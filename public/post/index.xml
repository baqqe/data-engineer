<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Lucas Bagge</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Isabella Benabaye, 2021</copyright><lastBuildDate>Tue, 29 Dec 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>/img/sharing_image.jpg</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>Text mining - nøie trust pilot reviews</title>
      <link>/post/text-mining-n%C3%B8ie-trust-pilot-reviews/</link>
      <pubDate>Tue, 29 Dec 2020 00:00:00 +0000</pubDate>
      <guid>/post/text-mining-n%C3%B8ie-trust-pilot-reviews/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this analysis I am gonna scapes truspilot web page for reviews given by
customer for the skincare firm Nøie.&lt;/p&gt;
&lt;p&gt;Here I am gonna use the data to make some topic modelling.&lt;/p&gt;
&lt;h2 id=&#34;web-scraping&#34;&gt;Web scraping&lt;/h2&gt;
&lt;p&gt;For sciping the 
&lt;a href=&#34;https://www.trustpilot.com/review/noie.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;trutpilot site&lt;/a&gt;
&lt;/p&gt;
&lt;p&gt;I am gonna make three functions: &lt;code&gt;get_ratings&lt;/code&gt;, &lt;code&gt;get_reviews&lt;/code&gt; and
&lt;code&gt;get_reviewer_names&lt;/code&gt; and combine it into a tibble with &lt;code&gt;get_data&lt;/code&gt; to extract the
data.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;get_ratings &amp;lt;- function(html) {
  html %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;body&amp;quot;) %&amp;gt;%
    html_nodes(&amp;quot;.star-rating&amp;quot;) %&amp;gt;%
    as.character() %&amp;gt;%
    str_subset(&amp;quot;medium&amp;quot;) %&amp;gt;%
    str_extract(&amp;quot;(\\d stjerne)&amp;quot;) %&amp;gt;%
    str_remove((&amp;quot;( stjerne)&amp;quot;)) %&amp;gt;%
    unlist()
}

get_reviews &amp;lt;- function(html) {
  html %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.review-content__body&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_trim() %&amp;gt;%
    unlist()
}

get_reviewer_names &amp;lt;- function(html) {
  html %&amp;gt;%
    read_html() %&amp;gt;%
    html_nodes(&amp;quot;.consumer-information__name&amp;quot;) %&amp;gt;%
    html_text() %&amp;gt;%
    str_trim() %&amp;gt;%
    unlist()
}

get_data &amp;lt;- function(html) {
  review &amp;lt;- get_reviews(html)
  names &amp;lt;- get_reviewer_names(html)
  ratings &amp;lt;- get_ratings(html)
  data &amp;lt;- tibble(
    reviewer = names,
    rating = ratings,
    review = review
  )
  data
}

urls &amp;lt;- cbind(c(url1, url2, url3, url4, url5, url6, url7))

url_list &amp;lt;- map(urls, get_data) %&amp;gt;%
  as.list()

data &amp;lt;- do.call(bind_rows, url_list)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let us quick take a look at what I have extracted from the site:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;data %&amp;gt;%
  head()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   reviewer                rating review                                         
##   &amp;lt;chr&amp;gt;                   &amp;lt;chr&amp;gt;  &amp;lt;chr&amp;gt;                                          
## 1 Lars Jensen             5      &amp;quot;Gode produkter og super service. Skru ned for…
## 2 Maria Cirkeline Rasmin… 5      &amp;quot;Super produkter\n                \n        \n…
## 3 Lis                     1      &amp;quot;2 cremer gav endnu mere uren hud + ulovlig ma…
## 4 Trine Holm              5      &amp;quot;Min hud slår altid ud om vinteren.\n         …
## 5 Camilla                 1      &amp;quot;Min hud er værre end nogensinde\n            …
## 6 Andrea Broe             5      &amp;quot;Stor anbefaling\n                \n        \n…
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see the following information:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;reviewer&lt;/code&gt; that is the person that has used the product.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;rating&lt;/code&gt; what the the reviewer has chosen to give the product on a scale from
1-5.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;review&lt;/code&gt; is the comment given by the reviewer and the central aspect for this
analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In the next section I will drewll into into the preprocessig step for this text
mining task.&lt;/p&gt;
&lt;h2 id=&#34;loading-and-preparing-the-data&#34;&gt;Loading and preparing the data&lt;/h2&gt;
&lt;p&gt;From the data we can see that the reviews are in Danish. Here we can use the &lt;code&gt;happyorsad&lt;/code&gt; package
to compute a sentiment score for each review. Thease score are based on a Danish list of
sentiment words and put toheather by 
&lt;a href=&#34;https://www.dtu.dk/service/telefonbog/person?id=1755&amp;amp;cpid=&amp;amp;tab=1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Finn Årup Nielsen&lt;/a&gt;
&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df &amp;lt;-
  data %&amp;gt;%
  mutate(sentiment = map_int(review, happyorsad, &amp;quot;da&amp;quot;)) %&amp;gt;%
  mutate(review = tolower(review)) %&amp;gt;%
  mutate(review = removeWords(
    review,
    c(
      &amp;quot;så&amp;quot;, &amp;quot;3&amp;quot;, &amp;quot;kan&amp;quot;, &amp;quot;få&amp;quot;, &amp;quot;får&amp;quot;, &amp;quot;fik&amp;quot;, &amp;quot;nøie&amp;quot;,
      stopwords(&amp;quot;danish&amp;quot;)
    )
  ))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;distribution-of-sentiment-scores&#34;&gt;Distribution of sentiment scores&lt;/h2&gt;
&lt;p&gt;In the density plot we see how sentiment scores are distributed with a median
score of 2. This a really good score and it is of interst to find out &lt;em&gt;why&lt;/em&gt;
Nøie has a this great score and it also svore 4.5 rating out of 5.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df %&amp;gt;%
  ggplot(aes(x = sentiment)) +
  geom_density(size = 1) +
  geom_vline(
    xintercept = median(df$sentiment),
    colour = &amp;quot;indianred&amp;quot;, linetype = &amp;quot;dashed&amp;quot;, size = 1
  ) +
  ggplot2::annotate(&amp;quot;text&amp;quot;,
    x = 15, y = 0.06,
    label = paste(&amp;quot;median = &amp;quot;, median(df$sentiment)), colour = &amp;quot;indianred&amp;quot;
  ) +
  my_theme() +
  xlim(-40, 40)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-text-mining-nøie-trust-pilot-reviews/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;In a crude way we can put positive and negative reviews in separate data frames
perform topic modelling on each in order to explore what reviewers lik and
dislike.&lt;/p&gt;
&lt;h2 id=&#34;topic-modelling-for-positive-reviews&#34;&gt;Topic modelling for positive reviews&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;df_pos &amp;lt;-
  df %&amp;gt;%
  filter(sentiment &amp;gt; 1) %&amp;gt;%
  unnest_tokens(word, review) %&amp;gt;%
  mutate(word = str_replace(word, &amp;quot;cremen&amp;quot;, &amp;quot;creme&amp;quot;)) %&amp;gt;%
  mutate(word = str_replace(word, &amp;quot;cremer&amp;quot;, &amp;quot;creme&amp;quot;)) %&amp;gt;%
  mutate(word = str_replace(word, &amp;quot;cremejeg&amp;quot;, &amp;quot;creme&amp;quot;)) %&amp;gt;%
  mutate(word = str_replace(word, &amp;quot;cremene&amp;quot;, &amp;quot;creme&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before creating a so called &lt;strong&gt;document term matrix&lt;/strong&gt; we need to count the
frequency of each word per document.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;words_pos &amp;lt;- df_pos %&amp;gt;%
  count(reviewer, word, sort = TRUE) %&amp;gt;%
  ungroup()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We want to use the famouse &lt;code&gt;Latent Dirichlet Allocation&lt;/code&gt; algorithme for topic
modelling. To use this we need to create our DTM and here we use &lt;code&gt;tidytext&lt;/code&gt; function
&lt;code&gt;cast_dtm&lt;/code&gt; to do that.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;reviewDTM_pos &amp;lt;- words_pos %&amp;gt;%
  cast_dtm(reviewer, word, n)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;LDA assumes that every document is a mixture of topics, and every topic is a
mixture of words. The k argument is used to specify the desired amount of topics
that we want in our model. Let´s create a two-topic mode.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;reviewLDA_pos &amp;lt;- LDA(reviewDTM_pos, k = 3, control = list(seed = 123))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The following table shows how many reviews that are assigned to each topic&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;tibble(topics(reviewLDA_pos)) %&amp;gt;%
  group_by(`topics(reviewLDA_pos)`) %&amp;gt;%
  count() %&amp;gt;%
  kable() %&amp;gt;%
  kable_styling(
    full_width = FALSE,
    position = &amp;quot;left&amp;quot;
  )
&lt;/code&gt;&lt;/pre&gt;
&lt;table class=&#34;table&#34; style=&#34;width: auto !important; &#34;&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; topics(reviewLDA_pos) &lt;/th&gt;
   &lt;th style=&#34;text-align:right;&#34;&gt; n &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 1 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 50 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 2 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 29 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 3 &lt;/td&gt;
   &lt;td style=&#34;text-align:right;&#34;&gt; 30 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;It is also possible to get the per-topic word probabilities or &amp;lsquo;beta&amp;rsquo;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;topics_pos &amp;lt;- tidy(reviewLDA_pos, matrix = &amp;quot;beta&amp;quot;)
topics_pos
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 3,336 x 3
##    topic term          beta
##    &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
##  1     1 creme     0.0611  
##  2     2 creme     0.0320  
##  3     3 creme     0.0499  
##  4     1 tak       0.000964
##  5     2 tak       0.00569 
##  6     3 tak       0.00600 
##  7     1 hud       0.0216  
##  8     2 hud       0.0229  
##  9     3 hud       0.0275  
## 10     1 produkter 0.0229  
## # … with 3,326 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now we can find the words with the highest beta. Here we choose the top five
words which we will show in a plot.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;top_terms_pos &amp;lt;- topics_pos %&amp;gt;%
  group_by(topic) %&amp;gt;%
  top_n(5, beta) %&amp;gt;%
  ungroup() %&amp;gt;%
  arrange(topic, -beta) %&amp;gt;%
  mutate(order = rev(row_number()))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;# plot_pos &amp;lt;-
top_terms_pos %&amp;gt;%
  ggplot(aes(order, beta)) +
  ggtitle(&amp;quot;Positive review topics&amp;quot;) +
  geom_col(show.legend = FALSE, fill = &amp;quot;steelblue&amp;quot;) +
  scale_x_continuous(
    breaks = top_terms_pos$order,
    labels = top_terms_pos$term,
    expand = c(0, 0)
  ) +
  facet_wrap(~topic, scales = &amp;quot;free&amp;quot;) +
  coord_flip(ylim = c(0, 0.02)) +
  my_theme() +
  theme(axis.title = element_blank())
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-text-mining-nøie-trust-pilot-reviews/index.en_files/figure-html/unnamed-chunk-14-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;h2 id=&#34;word-co-occurrence-within-reviews&#34;&gt;Word co-occurrence within reviews&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-r&#34;&gt;pairs_plot_pos &amp;lt;- word_pairs_pos &amp;lt;-
  df_pos %&amp;gt;%
  pairwise_count(word, reviewer, sort = TRUE) %&amp;gt;%
  filter(n &amp;gt;= 10) %&amp;gt;%
  graph_from_data_frame() %&amp;gt;%
  ggraph(layout = &amp;quot;fr&amp;quot;) +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = &amp;quot;steelblue&amp;quot;) +
  ggtitle(&amp;quot;Positive word pairs&amp;quot;) +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name),
    repel = TRUE,
    point.padding = unit(0.2, &amp;quot;lines&amp;quot;)
  ) +
  my_theme() +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank()
  )

grid.arrange(pairs_plot_pos)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-12-29-text-mining-nøie-trust-pilot-reviews/index.en_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodels: Decision Tree Learning in R</title>
      <link>/post/2020-06-02-tidymodels-decision-tree-learning-in-r/</link>
      <pubDate>Tue, 02 Jun 2020 00:00:00 +0000</pubDate>
      <guid>/post/2020-06-02-tidymodels-decision-tree-learning-in-r/</guid>
      <description>
&lt;link href=&#34;index_files/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;index_files/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;RStudio has recently released a cohesive suite of packages for modelling and machine learning, called &lt;code&gt;{tidymodels}&lt;/code&gt;. The successor to Max Kuhn’s &lt;code&gt;{caret}&lt;/code&gt; package, &lt;code&gt;{tidymodels}&lt;/code&gt; allows for a tidy approach to your data from start to finish. We’re going to walk through the basics for getting off the ground with &lt;code&gt;{tidymodels}&lt;/code&gt; and demonstrate its application to three different tree-based methods for predicting student test scores. For further information about the package, you can visit &lt;a href=&#34;https://www.tidymodels.org/&#34; class=&#34;uri&#34;&gt;https://www.tidymodels.org/&lt;/a&gt;.&lt;/p&gt;
&lt;div id=&#34;setup&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Setup&lt;/h1&gt;
&lt;p&gt;Load both the &lt;code&gt;{tidyverse}&lt;/code&gt; and &lt;code&gt;{tidymodels}&lt;/code&gt; packages into your environment. We’ll also load in the &lt;code&gt;{skimr}&lt;/code&gt; package to help us with some descriptives for our data and a host of other packages that will be required to run our machine learning models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidymodels) 
library(tidyverse) # manipulating data
library(skimr) # data visualization
library(baguette) # bagged trees
library(future) # parallel processing &amp;amp; decrease computation time
library(xgboost) # boosted trees&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;import-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Import the data&lt;/h1&gt;
&lt;p&gt;We use simulated data which approximates reading and math scores for ~189,000 3rd-8th grade students in Oregon public schools see &lt;a href=&#34;https://www.kaggle.com/c/edld-654-spring-2020&#34;&gt;this Kaggle page&lt;/a&gt; for details. For the purpose of demonstration, we’ll be sampling 1% of the data with &lt;code&gt;sample_frac()&lt;/code&gt; to keep computer processing time manageable. All school IDs in the data are real, so we can use that information to link the data with other sources. Specifically, we’re also going to pull in some data on student enrollment in free and reduced lunch from the National Center for Education Statistics and some ethnicity data from the Oregon Department of Education.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)

# import data and perform initial cleaning
# initial cleaning steps include: 
# *recode NA&amp;#39;s for lang_cd and ayp_lep to more meaningful values
# *remove vars with entirely missing data
# Note: the data is called &amp;#39;train.csv&amp;#39;, but we will actually further split this into its own training and testing data

dat &amp;lt;- read_csv(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;train.csv&amp;quot;)) %&amp;gt;% 
  select(-classification) %&amp;gt;% # remove this variable because it&amp;#39;s redundant with `score`
  mutate(lang_cd = ifelse(is.na(lang_cd), &amp;quot;E&amp;quot;, lang_cd), 
         ayp_lep = ifelse(is.na(ayp_lep), &amp;quot;G&amp;quot;, ayp_lep)) %&amp;gt;% 
  sample_frac(.01) %&amp;gt;% # sample 1% of the data to reduce run time
  janitor::remove_empty(c(&amp;quot;rows&amp;quot;, &amp;quot;cols&amp;quot;)) %&amp;gt;% 
  drop_na() %&amp;gt;% 
  select_if(~length(unique(.x)) &amp;gt; 1)

# import fall membership report ethcnicity data and do some basic cleaning and renaming
sheets &amp;lt;- readxl::excel_sheets(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;fallmembershipreport_20192020.xlsx&amp;quot;))

ode_schools &amp;lt;- readxl::read_xlsx(here::here(&amp;quot;static&amp;quot;, &amp;quot;data&amp;quot;, &amp;quot;fallmembershipreport_20192020.xlsx&amp;quot;),
                                 sheet = sheets[4])

ethnicities &amp;lt;- ode_schools %&amp;gt;%
  select(attnd_schl_inst_id = `Attending School ID`,
         attnd_dist_inst_id = `Attending District Institution ID`,
         sch_name = `School Name`,
         contains(&amp;quot;%&amp;quot;)) %&amp;gt;%
  janitor::clean_names()

names(ethnicities) &amp;lt;- gsub(&amp;quot;x2019_20_percent&amp;quot;, &amp;quot;p&amp;quot;, names(ethnicities))

# join ethnicity data with original dataset
dat &amp;lt;- left_join(dat, ethnicities)

# import and tidy free and reduced lunch data 
frl &amp;lt;- rio::import(&amp;quot;https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip&amp;quot;,
              setclass = &amp;quot;tbl_df&amp;quot;)  %&amp;gt;% 
  janitor::clean_names()  %&amp;gt;% 
  filter(st == &amp;quot;OR&amp;quot;)  %&amp;gt;%
  select(ncessch, lunch_program, student_count)  %&amp;gt;% 
  mutate(student_count = replace_na(student_count, 0))  %&amp;gt;% 
  pivot_wider(names_from = lunch_program,
              values_from = student_count)  %&amp;gt;% 
  janitor::clean_names()  %&amp;gt;% 
  mutate(ncessch = as.double(ncessch))

# import student counts for each school across grades
stu_counts &amp;lt;- rio::import(&amp;quot;https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv&amp;quot;, setclass = &amp;quot;tbl_df&amp;quot;)  %&amp;gt;% 
  filter(state == &amp;quot;OR&amp;quot; &amp;amp; year == 1718)  %&amp;gt;% 
  count(ncessch, wt = n)  %&amp;gt;% 
  mutate(ncessch = as.double(ncessch))

# join frl and stu_counts data
frl &amp;lt;- left_join(frl, stu_counts)

# add frl data to train data
dat &amp;lt;- left_join(dat, frl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After loading in our three datasets, we’ll join them together to make one cohesive data set to use for modelling. After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our &lt;code&gt;{tidymodels}&lt;/code&gt; tree-based examples.&lt;/p&gt;
&lt;p&gt;For a more complete description of the variables, you can download the data dictionary &lt;a href=&#34;data_dictionary.csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;explore-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Explore the data&lt;/h1&gt;
&lt;p&gt;We’ll use the &lt;code&gt;skim()&lt;/code&gt; function from &lt;code&gt;{skimr}&lt;/code&gt; to take a closer look at our variables. Many numeric predictors are clearly non-normal (see histograms below), but this is no problem as tree-based methods are robust to non-normality.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  select(-contains(&amp;quot;id&amp;quot;), -ncessch, -missing, -not_applicable) %&amp;gt;%  # remove ID and irrelevant variables
  mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %&amp;gt;% # covert test date to date
  modify_if(is.character, as.factor) %&amp;gt;%  # convert character vars to factors
  skim() %&amp;gt;% 
  select(-starts_with(&amp;quot;numeric.p&amp;quot;)) # remove quartiles&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-4&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Piped data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1857&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;41&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Date&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;factor&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;25&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: Date&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;median&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_dt&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-03-16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-06-07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;2018-05-18&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;47&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: factor&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;ordered&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;top_counts&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;gndr&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;M: 939, F: 918&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ethnic_cd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;W: 1151, H: 458, M: 100, A: 79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_bnch&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;G6: 343, 1B: 330, G4: 304, G7: 304&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;migrant_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1793, Y: 64&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ind_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1842, Y: 15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sp_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1614, Y: 243&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tag_ed_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1759, Y: 98&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;econ_dsvntg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1100, N: 757&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_lep&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;G: 1471, F: 164, Y: 72, E: 58&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;stay_in_dist&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1811, N: 46&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;stay_in_schl&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;dist_sped&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1846, Y: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;trgt_assist_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;N: 1773, Y: 83, y: 1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;ayp_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1785, N: 72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1803, N: 54&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rc_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1785, N: 72&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lang_cd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;E: 1815, S: 42&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tst_atmpt_fg&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1853, P: 4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_schl_partic&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1846, N: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_dist_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1845, N: 12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;grp_rpt_schl_prfrm&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Y: 1834, N: 23&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;sch_name&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;FALSE&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;699&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Hig: 14, Jud: 14, Hou: 13, Fiv: 11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;enrl_grd&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;5.44&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.69&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▃▅▃▃&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;score&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2495.34&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;115.19&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▁▂▇▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lat&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;44.79&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.99&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▂▁▂▅▇&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;lon&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;-122.51&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.16&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▅▇▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_american_indian_alaska_native&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_asian&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.07&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_native_hawaiian_pacific_islander&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.01&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_black_african_american&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.02&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.04&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_hispanic_latino&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.18&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▅▂▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_white&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.60&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.20&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▁▃▅▇▅&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;p_multiracial&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.06&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.03&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▆▁▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;free_lunch_qualified&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;231.23&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;147.55&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;reduced_price_lunch_qualified&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;39.86&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24.77&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▆▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;no_category_codes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;271.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;165.44&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▆▇▃▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;n&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;816.07&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;536.55&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▃▂▁▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;While most of our predictors are categorical, we can use &lt;code&gt;{corrplot}&lt;/code&gt; to better visualize the relationships among the numeric variables.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;dat %&amp;gt;% 
  select(-contains(&amp;quot;id&amp;quot;), -ncessch, -missing, -not_applicable) %&amp;gt;% 
  select_if(is.numeric) %&amp;gt;% 
  select(score, everything()) %&amp;gt;% 
  cor(use = &amp;quot;complete.obs&amp;quot;) %&amp;gt;% 
  corrplot::corrplot()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-5-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;split-data-and-resample&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Split data and resample&lt;/h1&gt;
&lt;p&gt;The first step of our analysis is to split our data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (i.e., “tune”) the model’s hyperparameters before evaluating its final performance on our test data. By allowing us to test a model on a new sample, we assess “out of sample” accuracy (i.e., unseen data-—what all predictive models are interested in) and limit overfitting to the training set. We can do this efficiently with the &lt;code&gt;initial_split()&lt;/code&gt; function. This comes from the &lt;code&gt;{rsample}&lt;/code&gt; package, which is part of the &lt;code&gt;{tidymodels}&lt;/code&gt; package that we already loaded. Defaults put 75% of the data in the training set and 25% in the test set, but this can be adjusted with the &lt;code&gt;prop&lt;/code&gt; argument. Then, we’ll extract the training data from our split object and assign it a name.&lt;/p&gt;
&lt;p&gt;To further prevent over-fitting, we’ll resample our data using &lt;code&gt;vfold_cv()&lt;/code&gt;. This function outputs k-&lt;em&gt;fold&lt;/em&gt; cross-validated versions of our training data, where k = the number of times we resample (unsure why v- is used instead of k- here). By using k = 10 data sets, we get a better estimate of the model’s out-of-sample accuracy. On top of decreasing bias from over-fitting, this is essential when tuning hyperparameters (though we plan to apply defaults and not tune here, for brevity). Though our use of 10-fold cross validation is both frequently used and effective, it should be noted that other methods (e.g., bootstrap resampling) or other k-values are sometimes used to accomplish the same goal.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# split the data
split &amp;lt;- initial_split(dat)

# extract the training data
train &amp;lt;- training(split)

# resample the data with 10-fold cross-validation (10-fold by default)
cv &amp;lt;- vfold_cv(train)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pre-processing&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Pre-processing&lt;/h1&gt;
&lt;p&gt;Before we add in our data to the model, we’re going to set up an object that pre-processes our data. This is called a &lt;em&gt;recipe&lt;/em&gt;. To create a recipe, you’ll first specify a formula for your model, indicating which variable is your outcome and which are your predictors. Using &lt;code&gt;~.&lt;/code&gt; here will indicate that we want to use all variables other than &lt;code&gt;score&lt;/code&gt; as predictors. Then, we can specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role or performs feature engineering steps. Pre-processing may be sound uncommon, but if you’ve ever used &lt;code&gt;lm()&lt;/code&gt; (or several other &lt;code&gt;R&lt;/code&gt; functions) you’ve done some of this by simply calling the function (e.g., automatic dummy-coding to handle categorical data). This is beneficial because it gives the analyst more control, despite adding complexity to the process.&lt;/p&gt;
&lt;p&gt;A complete list of possible pre-processing steps can be found here: &lt;a href=&#34;https://www.tidymodels.org/find/recipes/&#34; class=&#34;uri&#34;&gt;https://www.tidymodels.org/find/recipes/&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;rec &amp;lt;- recipe(score ~ ., train) %&amp;gt;% 
  step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %&amp;gt;% # convert `test date` variable to a date 
  update_role(contains(&amp;quot;id&amp;quot;), ncessch, new_role = &amp;quot;id vars&amp;quot;) %&amp;gt;% # declare ID variables
  step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %&amp;gt;% # remove variables with zero variances
  step_novel(all_nominal()) %&amp;gt;% # prepares test data to handle previously unseen factor levels 
  step_unknown(all_nominal()) %&amp;gt;% # categorizes missing categorical data (NA&amp;#39;s) as `unknown`
  step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&amp;quot;id vars&amp;quot;))  %&amp;gt;% # replaces missing numeric observations with the median
  step_dummy(all_nominal(), -has_role(&amp;quot;id vars&amp;quot;)) # dummy codes categorical variables&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-model&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create a model&lt;/h1&gt;
&lt;p&gt;The last step before bringing in our data is to specify our model. This will call upon functions from the &lt;code&gt;{parsnip}&lt;/code&gt; package, which standardizes language for specifying a multitude of statistical models. There are a few core elements that you will need to specify for each model&lt;/p&gt;
&lt;div id=&#34;the-type-of-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The type of model&lt;/h2&gt;
&lt;p&gt;This indicates what type of model you choose to fit, each of which will be a different function. We’ll be focusing on decision tree methods using &lt;code&gt;bag_tree()&lt;/code&gt;, &lt;code&gt;random_forest()&lt;/code&gt;, and &lt;code&gt;boost_tree()&lt;/code&gt;. A full list of models can be found here &lt;a href=&#34;https://www.tidymodels.org/find/parsnip/&#34; class=&#34;uri&#34;&gt;https://www.tidymodels.org/find/parsnip/&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-engine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The engine&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_engine()&lt;/code&gt; calls the package to support the model you specified above.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-mode&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The mode&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_mode()&lt;/code&gt; indicates the type of prediction you’d like to use in your model, you’ll choose between regression and classification. Since we are looking to predict student scores, which is a continuous predictor, we’ll be choosing regression.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-arguments&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The arguments&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;set_args()&lt;/code&gt; allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered. For these parameters, you can either set a particular value or you can use the tune function to search for the optimal value of each parameter. Tuning requires a few extra steps, so we will leave the default arguments for clarity. For more information on tuning check out &lt;a href=&#34;https://tune.tidymodels.org/&#34; class=&#34;uri&#34;&gt;https://tune.tidymodels.org/&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;create-a-workflow&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Create a workflow&lt;/h1&gt;
&lt;p&gt;Up to this point we’ve been setting up a lot of individual elements and now it is time to combine them to create a cohesive framework, called a &lt;em&gt;workflow&lt;/em&gt;, so we can run our desired models. First, we’ll use the &lt;code&gt;workflow()&lt;/code&gt; command and then we’ll pulling the recipe and model we already created. The next section shows three examples of specifying models and creating a workflow for different decision tree methods.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-examples&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Model Examples&lt;/h1&gt;
&lt;div id=&#34;bagged-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bagged trees&lt;/h2&gt;
&lt;p&gt;A bagged tree approach creates multiple subsets of data from the training set which are randomly chosen with replacement. Each subset of data is used to train a given decision tree. In the end, we have an ensemble of different models. The predictions from all the different trees are averaged together, giving us a stronger prediction than one tree could independently.&lt;/p&gt;
&lt;div id=&#34;specify-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
mod_bag &amp;lt;- bag_tree() %&amp;gt;%
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;%
  set_engine(&amp;quot;rpart&amp;quot;, times = 10) # 10 bootstrap resamples&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_bag &amp;lt;- workflow() %&amp;gt;% 
  add_recipe(rec) %&amp;gt;%
  add_model(mod_bag)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_bag &amp;lt;- fit_resamples(
  wflow_bag,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) extract_model(x)))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;The plot below shows the root nodes from a bagged tree made of 100 trees (10 folds x 10 bootstrapped resamples). Root nodes are the 1st node in a decision tree, and they are determined by which variable best optimizes a loss function (e.g., minimizes mean square error [MSE] for continuous outcomes or Gini Index for categorical outcomes). Put roughly, the most common root nodes can be thought of as the most “important” predictors.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract roots
bag_roots &amp;lt;-  function(x){
  x %&amp;gt;% 
  select(.extracts) %&amp;gt;% 
  unnest(cols = c(.extracts)) %&amp;gt;% 
  mutate(models = map(.extracts,
                  ~.x$model_df)) %&amp;gt;% 
  select(-.extracts) %&amp;gt;% 
  unnest(cols = c(models)) %&amp;gt;% 
  mutate(root = map_chr(model,
                     ~as.character(.x$fit$frame[1, 1]))) %&amp;gt;%
  select(root)  
}

# plot
bag_roots(fit_bag) %&amp;gt;% 
  ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + 
  geom_bar() + 
  coord_flip() + 
  labs(x = &amp;quot;root&amp;quot;, y = &amp;quot;count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-11-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest&lt;/h2&gt;
&lt;p&gt;Random forest is similar to bagged tree methodology but goes one step further. In addition to taking random subsets of data, the model also draws a random selection of features. Instead of utilizing all features, the random subset of features allows more predictors to be eligible root nodes. This is particularly useful for handling high dimensionality data (e.g., have more variables than participants/cases).&lt;/p&gt;
&lt;div id=&#34;specify-the-model&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
mod_rf &amp;lt;-rand_forest() %&amp;gt;%
  set_engine(&amp;quot;ranger&amp;quot;,
             num.threads = parallel::detectCores(), 
             importance = &amp;quot;permutation&amp;quot;, 
             verbose = TRUE) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;) %&amp;gt;% 
  set_args(trees = 1000)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_rf &amp;lt;- workflow() %&amp;gt;% 
  add_model(mod_rf) %&amp;gt;% 
  add_recipe(rec)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_rf &amp;lt;- fit_resamples(
  wflow_rf,
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE,
                              extract = function(x) x)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;The plot below shows the root nodes from a random forest with 1000 trees (specified using &lt;code&gt;set_args(trees = 1000)&lt;/code&gt; in the parsnip model object).&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# extract roots
rf_tree_roots &amp;lt;- function(x){
  map_chr(1:1000, 
           ~ranger::treeInfo(x, tree = .)[1, &amp;quot;splitvarName&amp;quot;])
}

rf_roots &amp;lt;- function(x){
  x %&amp;gt;% 
  select(.extracts) %&amp;gt;% 
  unnest(cols = c(.extracts)) %&amp;gt;% 
  mutate(fit = map(.extracts,
                   ~.x$fit$fit$fit),
         oob_rmse = map_dbl(fit,
                         ~sqrt(.x$prediction.error)),
         roots = map(fit, 
                        ~rf_tree_roots(.))
         ) %&amp;gt;% 
  select(roots) %&amp;gt;% 
  unnest(cols = c(roots))
}

# plot
rf_roots(fit_rf) %&amp;gt;% 
  group_by(roots) %&amp;gt;% 
  count() %&amp;gt;% 
  arrange(desc(n)) %&amp;gt;% 
  filter(n &amp;gt; 75) %&amp;gt;% 
  ggplot(aes(fct_reorder(roots, n), n)) +
           geom_col() + 
           coord_flip() + 
  labs(x = &amp;quot;root&amp;quot;, y = &amp;quot;count&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;index_files/figure-html/unnamed-chunk-15-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;boosted-trees&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Boosted trees&lt;/h2&gt;
&lt;p&gt;Boosted trees, like bagged trees, are an ensemble model. Instead of applying successive models to resampled data and pooling estimates, boosted trees fit the next tree to the residuals (i.e., error term) of the prior tree. The goal is to minimize residual error through multiple trees, and is typically done with fairly “shallow” decision tree (i.e., 1-6 splits in each tree). Though each model is only slightly improving the error rate, the sequential use of many shallow trees makes computationally efficient (i.e. reduced run time) and highly accurate predictions.&lt;/p&gt;
&lt;div id=&#34;specify-the-model-1&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Specify the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;mod_boost &amp;lt;- boost_tree() %&amp;gt;% 
  set_engine(&amp;quot;xgboost&amp;quot;, nthreads = parallel::detectCores()) %&amp;gt;% 
  set_mode(&amp;quot;regression&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;create-workflow-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Create workflow&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;wflow_boost &amp;lt;- workflow() %&amp;gt;% 
  add_recipe(rec) %&amp;gt;% 
  add_model(mod_boost)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-the-model-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Fit the model&lt;/h3&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(100)
plan(multisession)

fit_boost &amp;lt;- fit_resamples(
  wflow_boost, 
  cv,
  metrics = metric_set(rmse, rsq),
  control = control_resamples(verbose = TRUE,
                              save_pred = TRUE)
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;visualize-2&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Visualize&lt;/h3&gt;
&lt;p&gt;One of the few downfalls of &lt;code&gt;{tidymodels}&lt;/code&gt; is its (current) inability to plot these tree-based models. For the past two models, it was simpler to extract root nodes and plot them, but their interpretation (as we’re fitting to residuals instead of data sets) are not straightforward. For that reason, we don’t have any pretty plots here. Instead, we’ll skip to evaluating the metrics of all models.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;evaluate-metrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Evaluate metrics&lt;/h1&gt;
&lt;p&gt;After running these three models, it’s time to evaluate their performance. We can do this with &lt;code&gt;tune::collect_metrics()&lt;/code&gt;. The table below shows the estimate of the out-of-sample performance for each of our 3 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;collect_metrics(fit_bag) %&amp;gt;% 
  bind_rows(collect_metrics(fit_rf)) %&amp;gt;%
  bind_rows(collect_metrics(fit_boost)) %&amp;gt;% 
  filter(.metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% 
  mutate(model = c(&amp;quot;bag&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;boost&amp;quot;)) %&amp;gt;% 
  select(model, everything()) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.estimator&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;std_err&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bag&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;98.42890&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.504904&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rf&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95.20828&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3.466279&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;boost&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;95.35888&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2.764773&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Here, we are faced with a common problem in the machine learning world: choosing between models that perform similarly (see overlapping standard errors). Whether we would prefer random forests or bagged trees may depend on computational efficiency (i.e., time) or other factors. In practice, tuning several hyperparameters may have made one model clearly preferable over the others, but in our case - relying on all defaults - we would probably have similar performance with both models on a new data set and would prefer random forest or boosted tree models for their efficiency.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;out-of-sample-performance&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Out-of-sample performance&lt;/h1&gt;
&lt;p&gt;The final step is to apply each trained model to our test data using &lt;code&gt;last_fit()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# bagged trees
final_fit_bag &amp;lt;- last_fit(
  wflow_bag,
  split = split
)

# random forest
final_fit_rf &amp;lt;- last_fit(
  wflow_rf,
  split = split
)

# boosted trees
final_fit_boost &amp;lt;- last_fit(
  wflow_boost,
  split = split
)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The table below shows the actual out-of-sample performance for each of our 3 models.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# show performance on test data
collect_metrics(final_fit_bag) %&amp;gt;% 
  bind_rows(collect_metrics(final_fit_rf)) %&amp;gt;%
  bind_rows(collect_metrics(final_fit_boost)) %&amp;gt;% 
  filter(.metric == &amp;quot;rmse&amp;quot;) %&amp;gt;% 
  mutate(model = c(&amp;quot;bag&amp;quot;, &amp;quot;rf&amp;quot;, &amp;quot;boost&amp;quot;)) %&amp;gt;% 
  select(model, everything()) %&amp;gt;% 
  knitr::kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;model&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;.estimator&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;.estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;bag&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;93.36504&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;rf&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;91.18114&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;boost&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;rmse&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;standard&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;94.22609&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;After applying our 3 trained models to the unseen test data, it looks like random forest is the winner since it has the lowest RMSE. In this example, we only used 1% of the data to train these models, which could make it difficult to meaningfully compare their performance. However, the random forest model also results in the best out-of-sample prediction (RMSE = 83.47).&lt;/p&gt;
&lt;details&gt;
&lt;p&gt;&lt;summary&gt;Session Info&lt;/summary&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## ─ Session info ───────────────────────────────────────────────────────────────
##  setting  value                       
##  version  R version 4.0.2 (2020-06-22)
##  os       macOS Catalina 10.15.7      
##  system   x86_64, darwin17.0          
##  ui       X11                         
##  language (EN)                        
##  collate  da_DK.UTF-8                 
##  ctype    da_DK.UTF-8                 
##  tz       Europe/Copenhagen           
##  date     2020-11-24                  
## 
## ─ Packages ───────────────────────────────────────────────────────────────────
##  package       * version    date       lib source        
##  assertthat      0.2.1      2019-03-21 [1] CRAN (R 4.0.2)
##  backports       1.1.8      2020-06-17 [1] CRAN (R 4.0.2)
##  baguette      * 0.1.0      2020-10-28 [1] CRAN (R 4.0.2)
##  base64enc       0.1-3      2015-07-28 [1] CRAN (R 4.0.2)
##  blob            1.2.1      2020-01-20 [1] CRAN (R 4.0.2)
##  blogdown        0.21       2020-10-11 [1] CRAN (R 4.0.2)
##  bookdown        0.20       2020-06-23 [1] CRAN (R 4.0.2)
##  broom         * 0.7.0      2020-07-09 [1] CRAN (R 4.0.2)
##  C50             0.1.3.1    2020-05-26 [1] CRAN (R 4.0.2)
##  cellranger      1.1.0      2016-07-27 [1] CRAN (R 4.0.2)
##  class           7.3-17     2020-04-26 [1] CRAN (R 4.0.2)
##  cli             2.0.2      2020-02-28 [1] CRAN (R 4.0.2)
##  codetools       0.2-16     2018-12-24 [1] CRAN (R 4.0.2)
##  colorspace      1.4-1      2019-03-18 [1] CRAN (R 4.0.2)
##  corrplot        0.84       2017-10-16 [1] CRAN (R 4.0.2)
##  crayon          1.3.4      2017-09-16 [1] CRAN (R 4.0.2)
##  Cubist          0.2.3      2020-01-10 [1] CRAN (R 4.0.2)
##  data.table      1.13.0     2020-07-24 [1] CRAN (R 4.0.2)
##  DBI             1.1.0      2019-12-15 [1] CRAN (R 4.0.2)
##  dbplyr          1.4.4      2020-05-27 [1] CRAN (R 4.0.2)
##  dials         * 0.0.9      2020-09-16 [1] CRAN (R 4.0.2)
##  DiceDesign      1.8-1      2019-07-31 [1] CRAN (R 4.0.2)
##  digest          0.6.25     2020-02-23 [1] CRAN (R 4.0.2)
##  dplyr         * 1.0.2      2020-08-18 [1] CRAN (R 4.0.2)
##  earth           5.3.0      2020-10-11 [1] CRAN (R 4.0.2)
##  ellipsis        0.3.1      2020-05-15 [1] CRAN (R 4.0.2)
##  evaluate        0.14       2019-05-28 [1] CRAN (R 4.0.1)
##  fansi           0.4.1      2020-01-08 [1] CRAN (R 4.0.2)
##  farver          2.0.3      2020-01-16 [1] CRAN (R 4.0.2)
##  forcats       * 0.5.0      2020-03-01 [1] CRAN (R 4.0.2)
##  foreach         1.5.1      2020-10-15 [1] CRAN (R 4.0.2)
##  Formula         1.2-4      2020-10-16 [1] CRAN (R 4.0.2)
##  fs              1.5.0      2020-07-31 [1] CRAN (R 4.0.2)
##  furrr           0.2.1      2020-10-21 [1] CRAN (R 4.0.2)
##  future        * 1.20.1     2020-11-03 [1] CRAN (R 4.0.2)
##  generics        0.1.0      2020-10-31 [1] CRAN (R 4.0.2)
##  ggplot2       * 3.3.2      2020-06-19 [1] CRAN (R 4.0.2)
##  globals         0.13.1     2020-10-11 [1] CRAN (R 4.0.2)
##  glue            1.4.1      2020-05-13 [1] CRAN (R 4.0.2)
##  gower           0.2.2      2020-06-23 [1] CRAN (R 4.0.2)
##  GPfit           1.0-8      2019-02-08 [1] CRAN (R 4.0.2)
##  gtable          0.3.0      2019-03-25 [1] CRAN (R 4.0.2)
##  hardhat         0.1.5      2020-11-09 [1] CRAN (R 4.0.2)
##  haven           2.3.1      2020-06-01 [1] CRAN (R 4.0.2)
##  highr           0.8        2019-03-20 [1] CRAN (R 4.0.2)
##  hms             0.5.3      2020-01-08 [1] CRAN (R 4.0.2)
##  htmltools       0.5.0      2020-06-16 [1] CRAN (R 4.0.2)
##  httr            1.4.2      2020-07-20 [1] CRAN (R 4.0.2)
##  infer         * 0.5.3      2020-07-14 [1] CRAN (R 4.0.2)
##  inum            1.0-1      2019-04-25 [1] CRAN (R 4.0.2)
##  ipred           0.9-9      2019-04-28 [1] CRAN (R 4.0.2)
##  iterators       1.0.13     2020-10-15 [1] CRAN (R 4.0.2)
##  jsonlite        1.7.0      2020-06-25 [1] CRAN (R 4.0.2)
##  knitr           1.29       2020-06-23 [1] CRAN (R 4.0.2)
##  labeling        0.3        2014-08-23 [1] CRAN (R 4.0.2)
##  lattice         0.20-41    2020-04-02 [1] CRAN (R 4.0.2)
##  lava            1.6.8.1    2020-11-04 [1] CRAN (R 4.0.2)
##  lhs             1.1.1      2020-10-05 [1] CRAN (R 4.0.2)
##  libcoin         1.0-6      2020-08-14 [1] CRAN (R 4.0.2)
##  lifecycle       0.2.0      2020-03-06 [1] CRAN (R 4.0.2)
##  listenv         0.8.0      2019-12-05 [1] CRAN (R 4.0.2)
##  lubridate       1.7.9      2020-06-08 [1] CRAN (R 4.0.2)
##  magrittr        1.5        2014-11-22 [1] CRAN (R 4.0.2)
##  MASS            7.3-52     2020-08-18 [1] CRAN (R 4.0.2)
##  Matrix          1.2-18     2019-11-27 [1] CRAN (R 4.0.2)
##  modeldata     * 0.1.0      2020-10-22 [1] CRAN (R 4.0.2)
##  modelr          0.1.8      2020-05-19 [1] CRAN (R 4.0.2)
##  munsell         0.5.0      2018-06-12 [1] CRAN (R 4.0.2)
##  mvtnorm         1.1-1      2020-06-09 [1] CRAN (R 4.0.2)
##  nnet            7.3-14     2020-04-26 [1] CRAN (R 4.0.2)
##  parallelly      1.21.0     2020-10-27 [1] CRAN (R 4.0.2)
##  parsnip       * 0.1.4      2020-10-27 [1] CRAN (R 4.0.2)
##  partykit        1.2-10     2020-10-12 [1] CRAN (R 4.0.2)
##  pillar          1.4.6      2020-07-10 [1] CRAN (R 4.0.2)
##  pkgconfig       2.0.3      2019-09-22 [1] CRAN (R 4.0.2)
##  plotmo          3.6.0      2020-09-13 [1] CRAN (R 4.0.2)
##  plotrix         3.7-8      2020-04-16 [1] CRAN (R 4.0.2)
##  plyr            1.8.6      2020-03-03 [1] CRAN (R 4.0.2)
##  pROC            1.16.2     2020-03-19 [1] CRAN (R 4.0.2)
##  prodlim         2019.11.13 2019-11-17 [1] CRAN (R 4.0.2)
##  purrr         * 0.3.4      2020-04-17 [1] CRAN (R 4.0.2)
##  R6              2.4.1      2019-11-12 [1] CRAN (R 4.0.2)
##  ranger          0.12.1     2020-01-10 [1] CRAN (R 4.0.2)
##  Rcpp            1.0.5      2020-07-06 [1] CRAN (R 4.0.2)
##  readr         * 1.3.1      2018-12-21 [1] CRAN (R 4.0.2)
##  readxl          1.3.1      2019-03-13 [1] CRAN (R 4.0.2)
##  recipes       * 0.1.15     2020-11-11 [1] CRAN (R 4.0.2)
##  repr            1.1.0      2020-01-28 [1] CRAN (R 4.0.2)
##  reprex          0.3.0      2019-05-16 [1] CRAN (R 4.0.2)
##  reshape2        1.4.4      2020-04-09 [1] CRAN (R 4.0.2)
##  rlang           0.4.7      2020-07-09 [1] CRAN (R 4.0.2)
##  rmarkdown       2.5        2020-10-21 [1] CRAN (R 4.0.2)
##  rpart           4.1-15     2019-04-12 [1] CRAN (R 4.0.2)
##  rsample       * 0.0.8      2020-09-23 [1] CRAN (R 4.0.2)
##  rstudioapi      0.11       2020-02-07 [1] CRAN (R 4.0.2)
##  rvest           0.3.6      2020-07-25 [1] CRAN (R 4.0.2)
##  scales        * 1.1.1      2020-05-11 [1] CRAN (R 4.0.2)
##  sessioninfo     1.1.1      2018-11-05 [1] CRAN (R 4.0.2)
##  skimr         * 2.1.2      2020-07-06 [1] CRAN (R 4.0.2)
##  stringi         1.4.6      2020-02-17 [1] CRAN (R 4.0.2)
##  stringr       * 1.4.0      2019-02-10 [1] CRAN (R 4.0.2)
##  survival        3.2-3      2020-06-13 [1] CRAN (R 4.0.2)
##  TeachingDemos   2.12       2020-04-07 [1] CRAN (R 4.0.2)
##  tibble        * 3.0.3      2020-07-10 [1] CRAN (R 4.0.2)
##  tidymodels    * 0.1.1      2020-07-14 [1] CRAN (R 4.0.2)
##  tidyr         * 1.1.1      2020-07-31 [1] CRAN (R 4.0.2)
##  tidyselect      1.1.0      2020-05-11 [1] CRAN (R 4.0.2)
##  tidyverse     * 1.3.0      2019-11-21 [1] CRAN (R 4.0.2)
##  timeDate        3043.102   2018-02-21 [1] CRAN (R 4.0.2)
##  tune          * 0.1.2      2020-11-17 [1] CRAN (R 4.0.2)
##  vctrs           0.3.2      2020-07-15 [1] CRAN (R 4.0.2)
##  withr           2.2.0      2020-04-20 [1] CRAN (R 4.0.2)
##  workflows     * 0.2.1      2020-10-08 [1] CRAN (R 4.0.2)
##  xfun            0.19       2020-10-30 [1] CRAN (R 4.0.2)
##  xgboost       * 1.2.0.1    2020-09-02 [1] CRAN (R 4.0.2)
##  xml2            1.3.2      2020-04-23 [1] CRAN (R 4.0.2)
##  yaml            2.2.1      2020-02-01 [1] CRAN (R 4.0.2)
##  yardstick     * 0.0.7      2020-07-13 [1] CRAN (R 4.0.2)
## 
## [1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Tidymodels, Random Forest og parsnip</title>
      <link>/post/tidymodels-and-parsnip/</link>
      <pubDate>Mon, 17 Feb 2020 00:00:00 +0000</pubDate>
      <guid>/post/tidymodels-and-parsnip/</guid>
      <description>
&lt;link href=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.css&#34; rel=&#34;stylesheet&#34; /&gt;
&lt;script src=&#34;/rmarkdown-libs/anchor-sections/anchor-sections.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;introduktion&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Introduktion&lt;/h2&gt;
&lt;p&gt;I denne post vil jeg kigger på to modeller.
Den &lt;strong&gt;logistiske regression&lt;/strong&gt; og &lt;strong&gt;random forest&lt;/strong&gt;, hvor de begge bliver brugtblog
som klassifikations modeller.&lt;/p&gt;
&lt;p&gt;Jeg kommer til at gennemgå og beskrive Random Forest da det er en model,
som er forholdsvis ny for mig og der er nogle teoretiske framwork jeg gerne
vil prøve at forklare.&lt;/p&gt;
&lt;p&gt;Desuden kommer vi til at stifte bekendtskab med &lt;code&gt;parsnip&lt;/code&gt; som gør det let at
skifte om til de forskellige modeller. Med det nye framwork fra
&lt;code&gt;tidymodels&lt;/code&gt; kan man skifte utrolig let fra &lt;code&gt;glm&lt;/code&gt; til en &lt;strong&gt;cross validated&lt;/strong&gt;
random forest med &lt;code&gt;ranger&lt;/code&gt;
med få linjers koder.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Random forest&lt;/h2&gt;
&lt;p&gt;Det er en af de mest populære machine learning algoritmer og kan både bruges
som en regresssion og klassifikation model.&lt;/p&gt;
&lt;p&gt;Som navnet antyder så laver algoritmen en skov med forskellige beslutningstræer.
Desto flere træer desto mere robust er modellen. Navnet random kommer grundet to koncepter&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Et randomiseret sample af trænings data, når man bygger hver enkelt træ.&lt;/li&gt;
&lt;li&gt;Et randomiseret subsæt af features, når man splitter noder.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Når vi træer hver træ så lærer den fra et random sample af data punkter.
Samples er trukket med erstatning, som kaldes &lt;strong&gt;bootstrapping&lt;/strong&gt;, som betyder
at et sample vil blive brugt flere gange i et enkelt træ. Ideen er at ved at
træne hver træ med forskellige samples, så vil vi få en lavere varians og
ikke få et højere bias.&lt;/p&gt;
&lt;p&gt;Ens prediction fås ved at tage gennmsnittet af predictor for hver beslutningstræ.
Denne procedure kaldes for &lt;strong&gt;bagging&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Fordele er man kan bruge den som klassifikation og regression. Den vil ikke overfitte.
Den kan håndtere store datasæt med mange dimensioner.&lt;/p&gt;
&lt;p&gt;Ulemper er den ikke er så god til regressioner. Den er ikke god til at forudsige.
Der er heller ikke meget kontrol over modellen.&lt;/p&gt;
&lt;p&gt;Dog er modellen anvendelig i mange sektor såsom banker, forsikringsselskaber,
forretninger somkan bruges til at finde de loyolae kunder. Den kan også bruges i
aktiemarkedet til ast finde opførelsen af en aktie.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Data&lt;/h2&gt;
&lt;p&gt;I dette projekt bruger jeg data
fra Telco Customer Churn. Data indeholder 7043 rækker som hver repræsentere en kunde.
Der er 21 kolonner som er mulige predictor, der giver information til vi kan
forecast opførelse og give indsigt på forebyggelsesprogrammer.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Churn&lt;/code&gt; er den afhængige variable og viser om kunden har forladt virksomheden
indenfor den seneste måned.&lt;/p&gt;
&lt;p&gt;Jeg bruger funnktionen &lt;code&gt;skim&lt;/code&gt; til at skabe et overblik over mit data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;telco &amp;lt;- readr::read_csv(&amp;quot;https://raw.githubusercontent.com/treselle-systems/customer_churn_analysis/master/WA_Fn-UseC_-Telco-Customer-Churn.csv&amp;quot;)
telco %&amp;gt;% 
  skimr::skim()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;caption&gt;&lt;span id=&#34;tab:unnamed-chunk-1&#34;&gt;Table 1: &lt;/span&gt;Data summary&lt;/caption&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Name&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;Piped data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of rows&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;7043&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Number of columns&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;_______________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Column type frequency:&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;character&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;numeric&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;________________________&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Group variables&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;None&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: character&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;min&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;max&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;empty&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_unique&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;whitespace&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;customerID&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;10&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;7043&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;gender&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;6&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Partner&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Dependents&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PhoneService&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MultipleLines&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;InternetService&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;OnlineSecurity&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;OnlineBackup&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;DeviceProtection&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TechSupport&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;StreamingTV&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;StreamingMovies&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;19&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Contract&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;14&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PaperlessBilling&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;PaymentMethod&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;12&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;4&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;Churn&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Variable type: numeric&lt;/strong&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;skim_variable&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;n_missing&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;complete_rate&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;mean&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;sd&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p0&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p25&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p50&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p75&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;p100&lt;/th&gt;
&lt;th align=&#34;left&#34;&gt;hist&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;SeniorCitizen&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.16&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.37&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▁▁▁▂&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;tenure&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;32.37&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;24.56&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;9.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;29.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;55.00&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;72.00&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▃▃▃▆&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;MonthlyCharges&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;64.76&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;30.09&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.25&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;35.50&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;70.35&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;89.85&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;118.75&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▅▆▇▅&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;TotalCharges&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;11&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2283.30&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;2266.77&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;18.80&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;401.45&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;1397.47&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;3794.74&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;8684.80&lt;/td&gt;
&lt;td align=&#34;left&#34;&gt;▇▂▂▂▁&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Her er en række ting at lægge mærke til her.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;customerID&lt;/strong&gt; er en unik id for hver række og af den grund har den ingen
deskriptiv eller predictive power og den skal fjernes.&lt;/li&gt;
&lt;li&gt;Der er meget få &lt;strong&gt;NA&lt;/strong&gt; værdier, så de kan jeg tillade mig at slette.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;telco &amp;lt;- telco %&amp;gt;% 
  select(-customerID) %&amp;gt;% 
  drop_na()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;modellering-med-tidymodels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Modellering med &lt;code&gt;tidymodels&lt;/code&gt;&lt;/h2&gt;
&lt;p&gt;Denne post giver også en introduktion til tidymodels. Derfor vil modellen
være simpel og kommer til at bestå af &lt;strong&gt;logistic regression&lt;/strong&gt; model uden meget
data bearbejdring.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;train-and-test-split&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Train and test split&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;rsample()&lt;/code&gt; kan bruges til at lave en randomiserede træning og test data,
som selvfølgelig er konstrueret udfra vores orginale telco data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(1972)

train_test_split &amp;lt;- rsample::initial_split(
  data = telco,
  prop = 0.8
)
train_test_split&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## &amp;lt;Analysis/Assess/Total&amp;gt;
## &amp;lt;5626/1406/7032&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Ud fra ovenstående har vi at de 7032 kunder er blevet delt ud, og de 5626 er blevet
sat i træningssættet. Vi gemmer dem ned i deres eget data frame;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_tbl &amp;lt;- train_test_split %&amp;gt;% training() %&amp;gt;% 
  unnest()
test_tbl &amp;lt;- train_test_split %&amp;gt;% testing()&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;en-bage-opskrift&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;En bage opskrift&lt;/h2&gt;
&lt;p&gt;For at lave en del af arbejde for at bygge modellen bruger vi &lt;code&gt;recipe()&lt;/code&gt;. Denne
pakke bruger &lt;em&gt;bage metafor&lt;/em&gt; til at behandle data og foretage diverse præprocessor
såsom, missing values, fjerne predictor, centering og scaling osv..&lt;/p&gt;
&lt;p&gt;Det første man gør er at definere &lt;code&gt;recipe&lt;/code&gt; og de transformationer man vil bruge
på ens data. Der er ikke meget at gøre i dette tilfælde, udover at tranaformerer
til faktor.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recipe_simple &amp;lt;- function(dataset) {
  recipe(Churn ~ ., data = dataset) %&amp;gt;% 
    step_string2factor(all_nominal(), -all_outcomes()) %&amp;gt;% 
    prep(data = dataset)
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For at undgår man vi har en &lt;strong&gt;data lækage&lt;/strong&gt; (oveføre information mellem træning
og test data), skal data være ‘prepped’ ved
kun at bruge &lt;code&gt;train_tbl&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recipe_prepped &amp;lt;- recipe_simple(dataset = train_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Som den sidste del så skal vi &lt;em&gt;bage opskriften&lt;/em&gt; for at alle præprocessor
bliver inkluderet i data sættene.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;train_baked &amp;lt;- bake(recipe_prepped, new_data = train_tbl)
test_baked &amp;lt;- bake(recipe_prepped, new_data = test_tbl)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;fit-modellen&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fit modellen&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Tidymodels&lt;/code&gt; er det helt nye indspark fra tidyverse folkene på at skabe et framwork
for machine learning.
Hertil er der blevet lavet en del justeringer og nye pakker. En central pakke i
dette framwork er &lt;code&gt;parsnip&lt;/code&gt;,som skaber en adgang til mange machine learning pakker
uden man skal kunne syntaksen til dem alle.&lt;/p&gt;
&lt;p&gt;Man skal følge tre trin:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Bestem &lt;strong&gt;typen af modellen&lt;/strong&gt; og &lt;strong&gt;mode&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Bestem &lt;strong&gt;engine&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Bestem model specifikationer og data der skal bruges.&lt;/li&gt;
&lt;/ol&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;logistic_glm &amp;lt;- logistic_reg(mode = &amp;quot;classification&amp;quot;) %&amp;gt;% 
  set_engine(&amp;quot;glm&amp;quot;) %&amp;gt;% 
  fit(Churn ~ .,
      data = train_baked)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Som sagt så kan du vælge en masse andre engine. I dette tilfælde hvor vi bruge en
logistisk regression, så kan vi vælge; &lt;code&gt;glm&lt;/code&gt;, &lt;code&gt;glmnet&lt;/code&gt;, &lt;code&gt;stan&lt;/code&gt;, &lt;code&gt;spark&lt;/code&gt; og &lt;code&gt;keras&lt;/code&gt;.
Det smarte er vi bare kan skifte det ud og så klare parsnip transitionen.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;hvor-godt-klare-modellen-sig&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Hvor godt klare modellen sig?&lt;/h2&gt;
&lt;p&gt;Det er væsentlig at se hvor god modellen er og her bruger vi pakken
&lt;code&gt;yardstick&lt;/code&gt;, som gør det let at beregne forskellige måleværktøjer.
Før man kan beregne disse måle enheder skal vi beregne nogle
predictor ved at bruge &lt;code&gt;test_baked&lt;/code&gt; til predict funktionen.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prediction_glm &amp;lt;- logistic_glm %&amp;gt;% 
  predict(new_data = test_baked) %&amp;gt;%
  bind_cols(test_baked %&amp;gt;%  select(Churn))

head(prediction_glm)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   .pred_class Churn
##   &amp;lt;fct&amp;gt;       &amp;lt;fct&amp;gt;
## 1 Yes         No   
## 2 No          No   
## 3 No          No   
## 4 No          No   
## 5 No          No   
## 6 No          No&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Der kan benyttes mange matricer til at undersøge hvor god modellen er,
men fokus for denne post bliver &lt;strong&gt;accuracy&lt;/strong&gt;, &lt;strong&gt;precision&lt;/strong&gt;, &lt;strong&gt;recall&lt;/strong&gt; og &lt;strong&gt;F1_score&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Disse mål bliver udledt af &lt;strong&gt;Confusion Matrix&lt;/strong&gt;, som er en tabel der beskriver
hvor godt ens klassifikations model klarer sig. Denne matrice er i sig selv ikke svær at
forstå, da den angiver antallet af; &lt;em&gt;false positives&lt;/em&gt;, &lt;em&gt;false negatives&lt;/em&gt;, &lt;em&gt;true positives&lt;/em&gt;
og &lt;em&gt;true negatives&lt;/em&gt;. Dog er nogle af målene, som udledes herfra svære koncepter og kræver
reflektion for at forstå deres betydning.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prediction_glm %&amp;gt;% 
  conf_mat(Churn, .pred_class) %&amp;gt;% 
  pluck(1) %&amp;gt;% 
  as_tibble() %&amp;gt;% 
  ggplot(aes(Prediction, Truth, alpha = n)) +
  geom_tile(show.legend = FALSE) +
  geom_text(aes(label = n), colour = &amp;quot;white&amp;quot;, alpha = 0.5, size = 12)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2020-01-17-tidymodels-and-parship/index.en_files/figure-html/unnamed-chunk-10-1.png&#34; width=&#34;2400&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Modellen &lt;strong&gt;Accuracy&lt;/strong&gt; er andel af prediction modellen ramte plet og kan udregnes ved at
lade predictions_glm gå gennem metrics funktionen. Dog er den ikke så troværdig, hvis
ens data er ubalanceret.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prediction_glm %&amp;gt;% 
  metrics(Churn, .pred_class) %&amp;gt;% 
  select(-.estimator) %&amp;gt;% 
  filter(.metric == &amp;quot;accuracy&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 1 x 2
##   .metric  .estimate
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;
## 1 accuracy     0.806&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Modellen får altså en score på 78%.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Precision&lt;/strong&gt; målser hvor sensitiv modellen er overfor False Positive, mens
Recall ser hvor sensitiv modellen er for False Negative.&lt;/p&gt;
&lt;p&gt;Disse metricer er meget vigtig informationer for virksomheder fordi man så kan
forudsige hvilke kunder der er i en risiko gruppe for at forlade forretningen.
Herfra kan man så benytte sig af en fastholdessstrategi. Desuen kan
man bruge oplysning til ikke at bruge penge på kudner der alligevel
har tænkt sig at forlade virksomheden.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;tibble(
  &amp;quot;precision&amp;quot; =
    precision(prediction_glm, Churn, .pred_class) %&amp;gt;% 
    select(.estimate),
  &amp;quot;recall&amp;quot; =
    recall(prediction_glm, Churn, .pred_class) %&amp;gt;% 
    select(.estimate)
) %&amp;gt;% 
  unnest() %&amp;gt;% 
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;right&#34;&gt;precision&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;recall&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;right&#34;&gt;0.8466368&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9024857&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Den anden og sidste populær måleværktøj er F1_score, som er det harmoniske gennemsnit
af precision og recall. Den perfekte score på 1 fås når precision og recall er perfekte.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;prediction_glm %&amp;gt;%
  f_meas(Churn, .pred_class) %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;.estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;f_meas&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8736696&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/div&gt;
&lt;div id=&#34;fra-logitstik-regression-til-random-forest&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Fra logitstik regression til Random Forest&lt;/h2&gt;
&lt;p&gt;Det er utrolig simpel at skifte ens model ud med en anden. Den tidligere
anvendte logistisk regressions model kan vi hurtig skifte ud med en &lt;strong&gt;Random
Forest&lt;/strong&gt; model med &lt;code&gt;ranger&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;croos-validation-sæt-op&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Croos validation sæt op&lt;/h2&gt;
&lt;p&gt;For at styke modellens prediktive kræft kan man foretage cross validation, som
tit bliver sat op med 10 folder. Det kan implementeres med &lt;code&gt;vfold_cv()&lt;/code&gt; fra &lt;code&gt;rsample&lt;/code&gt;,
som splitter det initale trænings data.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;set.seed(123)
cross_val_tbl &amp;lt;- 
   vfold_cv(train_tbl, v = 10)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Vi kan genkende de 5626 fra vores tærningssæt. I hver runde vil 563 observationer
blive brugt til validere modellen for det specifikke fold.&lt;/p&gt;
&lt;p&gt;For at ikke blive forvirret over bruget af initial træsning/test split til det
man bruger i cross validation benytter man begreberne &lt;code&gt;analysis&lt;/code&gt; (estimer modellen)
og &lt;code&gt;assessment&lt;/code&gt; (valider estimater).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;opdater-recipe&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Opdater recipe&lt;/h2&gt;
&lt;p&gt;For at bruge Random Forest skal alle numeriske værdier være centred og scaled
og alle faktor skal være dummies.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;split &amp;lt;- initial_split(telco, prop = 0.8)
train_data &amp;lt;- training(split)
test_data &amp;lt;- testing(split)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;For at skifte over til en anden model er utroligt simepel. Her ændre vi til
random forest i typen af modellen og tilføjer dens hyperparameter.&lt;/p&gt;
&lt;p&gt;For at gøre processen lidt hurtigere propper jeg det hele i en funktion, som
estimer modellen på tværs af alle folder og retuner det i en tibble. Desuden skal
der tilføjes et skridt mere for at vi mapper de forskellige folder.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;recipe_rf &amp;lt;- function(dataset) {
  recipe(Churn ~ ., data = dataset) %&amp;gt;%
    step_string2factor(all_nominal(), -all_outcomes()) %&amp;gt;%
    step_dummy(all_nominal(), -all_outcomes()) %&amp;gt;%
    step_center(all_numeric()) %&amp;gt;%
    step_scale(all_numeric()) %&amp;gt;%
    prep(data = dataset)
}

rf_fun &amp;lt;- function(split, id, try, tree) {
   
  analysis_set &amp;lt;- split %&amp;gt;% analysis()
  analysis_prepped &amp;lt;- analysis_set %&amp;gt;% recipe_rf()
  analysis_baked &amp;lt;- analysis_prepped %&amp;gt;% bake(new_data = analysis_set)
  model_rf &amp;lt;-
    rand_forest(
      mode = &amp;quot;classification&amp;quot;,
      mtry = try,
      trees = tree
    ) %&amp;gt;%
    set_engine(&amp;quot;ranger&amp;quot;,
      importance = &amp;quot;impurity&amp;quot;
    ) %&amp;gt;%
    fit(Churn ~ ., data = analysis_baked)
  assessment_set &amp;lt;- split %&amp;gt;% assessment()
  assessment_prepped &amp;lt;- assessment_set %&amp;gt;% recipe_rf()
  assessment_baked &amp;lt;- assessment_prepped %&amp;gt;% bake(new_data = assessment_set)
  tibble(
    &amp;quot;id&amp;quot; = id,
    &amp;quot;truth&amp;quot; = assessment_baked$Churn,
    &amp;quot;prediction&amp;quot; = model_rf %&amp;gt;%
      predict(new_data = assessment_baked) %&amp;gt;%
      unlist()
  )
  
}

pred_rf &amp;lt;- map2_df(
  .x = cross_val_tbl$splits,
  .y = cross_val_tbl$id,
  ~ rf_fun(split = .x, id = .y, try = 3, tree = 200)
)
head(pred_rf)  &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   id     truth prediction
##   &amp;lt;chr&amp;gt;  &amp;lt;fct&amp;gt; &amp;lt;fct&amp;gt;     
## 1 Fold01 Yes   No        
## 2 Fold01 Yes   Yes       
## 3 Fold01 No    No        
## 4 Fold01 No    No        
## 5 Fold01 No    No        
## 6 Fold01 No    No&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pred_rf %&amp;gt;%
  conf_mat(truth, prediction) %&amp;gt;%
  summary() %&amp;gt;%
  select(-.estimator) %&amp;gt;%
  filter(.metric %in%
    c(&amp;quot;accuracy&amp;quot;, &amp;quot;precision&amp;quot;, &amp;quot;recall&amp;quot;, &amp;quot;f_meas&amp;quot;)) %&amp;gt;%
  kable()&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th align=&#34;left&#34;&gt;.metric&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;.estimate&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;accuracy&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.7996801&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;precision&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8291502&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td align=&#34;left&#34;&gt;recall&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.9147437&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td align=&#34;left&#34;&gt;f_meas&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;0.8698464&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Der er mange matricer til at validere vores model, men vi bruger dem som vi brugte
ved vores logistisk regression.&lt;/p&gt;
&lt;p&gt;Modellen klare sig på lige fod med regressionsmodellen. Man kunne gå tilbage til modellen
og laver yderligere feature eengierning da det ville gøre noget for selve
præcisionen af modellen.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
