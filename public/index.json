[{"authors":["admin"],"categories":null,"content":"I am a passionate Data Engineer that will love to give out of my knowledhe.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a passionate Data Engineer that will love to give out of my knowledhe.","tags":null,"title":"Lucas Bagge","type":"authors"},{"authors":[],"categories":["DBT","Data Modelling","SQL","Data Warehouse"],"content":" Introduktion ","date":1702771200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1702771200,"objectID":"ccc15e2e85c82de86164d7f65edb5007","permalink":"/dataengineering/dbt-intro/","publishdate":"2023-12-17T00:00:00Z","relpermalink":"/dataengineering/dbt-intro/","section":"dataengineering","summary":" Introduktion ","tags":["DBT","SQL"],"title":"DBT Introduction","type":"dataengineering"},{"authors":[],"categories":["Classification"],"content":"Introduction Amagerværket has two units 1 and 4 which produces heat and power using respectively wood pellets and wood chips as fuel. The fuel arrives to the plant by vessel and it is possible to track the arrivals through Copenhagen Malmö (CM) Port\u0026rsquo;s website. However, it is not directly possible to identify if the cargo of a given ship is either wood pellets or wood chips. Consequently, the objective is to build a model capable of classifying the vessels.\nWith the newly inaugurated wood chips fired unit 4 at Amagerværket and expectedly using well above 1 Mt of wood chips per year, the plant has suddenly\u0026mdash;and by a substantial margin\u0026mdash;become the largest player in the market for wood chips delivered by vessel. To maintain market visibility and have the best possible information to act in the biomass market, it is of importance to BIO Markets to have information about the arrival and consumption of biomass at Amagerværket.\nThe objective of this analysis is\nbuild a model that accurately and automatically classify vessels arriving to Amagerværket into groups according to cargo type\nThe data set For building our model we need to collect the cmport data . Here we have used web scraping techniques to extract the data that is essential for us to build our model.\nThe site is being updated on a regular basis, so our script that scrapes the data is running three times a day.\nAs a first analysis task we need to collect information about the port and if there were information on how we could classify the cargoes. We got great insight from colleagues that have tracked the cargoes for a long time and they gave us the information that quay, and arriving from could help us differentiated between the cargoes.\nWe need to explore the data for understanding it better and look at the outcome variable and the predictor we are considering being of most used for us.\ngross_feature_set %\u0026gt;% kbl() %\u0026gt;% kable_styling() feature description eta Estimated time of arrival etd Estimated time of departure name Vessel name company The shipping company carrying out the charter arriving_from The location from the vessel is arriving (not necessarily the loading port) nationality The country under whose laws the vessel is registered or licensed vessel_type The types os the vessel arriving purpose Give us the purpose of the vessel call_no CM Port call number length Length of the vessel width Width of the vessel brt Gross Register Tonnage (GRT) of the vessel imo The international Maritime Organization number quay The quay at Amagerværket to which the vessel is assigned The available dataset consists of \\(N = 129\\) observations of vessel arrivals\u0026mdash;also named port calls or just calls\u0026mdash;to Amagerværket each with 129 features. In the current context not every feature is important, but those deemed important for our model are:\ncargo: Specifies if the vessel carries wood chips (WC) or wood pellets (WP) arriving_from: The port/city where the cargo is being loaded quay: The quay at Amagerværket where the cargo is discharged The cargo variable is our outcome variable and the one we want to classify.\nThe cargo Here we can see that there is 105 ships with WC and WP with 24. We don´t have an equal amount of both cargoes and that is something that need to be considered in the preprocessing step.\nvessels %\u0026gt;% count(cargo) %\u0026gt;% ggplot(aes(x = cargo, y = n, fill = cargo)) + geom_col() + geom_text(aes(label = n), nudge_y = 3) + labs(y = NULL, x = NULL, title = \u0026quot;Dependent variable based on cargo\u0026quot;, subtitle = \u0026quot;Number of cargoes containing either WP or WC\u0026quot;) The quay at Amagerværket One important predictor is quay. It indicate where the vessel anchor at the port. Here we have four possibilities:\nc835 c836 c837 c838 tbl \u0026lt;- addmargins(table(vessels$cargo, vessels$quay)) tbl %\u0026gt;% as.data.frame() %\u0026gt;% pivot_wider(names_from = Var2, values_from = Freq) %\u0026gt;% kbl(col.names = c(\u0026quot;\u0026quot;, colnames(tbl)), caption = \u0026quot;Cross tabulation of cargo against quay.\u0026quot;) %\u0026gt;% kable_styling() From looking at the pattern in the data and talking to colleagues there is some of the quay that is a clear indicator of if the vessel is a WP or WC.\nvessels %\u0026gt;% count(quay) %\u0026gt;% ggplot(aes(x = quay, y = n, fill = quay)) + geom_col(position = \u0026quot;dodge\u0026quot;) + geom_text(aes(label = n), nudge_y = 3) + labs(y = NULL, x = NULL, title = \u0026quot;The Quay is a possible predictor\u0026quot;, subtitle = \u0026quot;Number of cargoes arriving to the different quay\u0026quot;) Here c838 is the quay where WP is being delivered and c836 plus c836 is mainly WC.\nvessels %\u0026gt;% count(quay, cargo) %\u0026gt;% ggplot(aes(reorder_within(quay, n, cargo), n, fill = quay)) + facet_grid(rows = vars(cargo), scales = 'free_y') + geom_col(position = \u0026quot;dodge\u0026quot;) + geom_text(aes(label = n), nudge_y = 1.9) + coord_flip() + scale_x_reordered() + labs(title = \u0026quot;The quay indicate what the cargo is delivering\u0026quot;, subtitle = \u0026quot;Number of cargoes arriving groped by WC or WP\u0026quot;, y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;) + theme(legend.title = element_blank()) From the above plots it is clear that we can from the quay seperat what the vessel is delivering. For c837 this can be considered a joker. It lies between the end quay, so our hypothesis is that it can be considered as a waiting quay where it can either be WC or WP.\nThe loading port of the cargo Another important predictor we want to include in the models framework is arriving_from. This indicate where the vessel is comming from. Here we want to make some investigation if we can use this as a predictor.\nvessels %\u0026gt;% group_by(arriving_from, cargo) %\u0026gt;% count(arriving_from, cargo) %\u0026gt;% ungroup() %\u0026gt;% mutate(across(where(is.character), as.factor)) %\u0026gt;% ggplot(aes(reorder_within(arriving_from, n, cargo), n, fill = cargo)) + geom_col(position = \u0026quot;dodge\u0026quot;) + geom_text(aes(label = n), nudge_y = 0.9) + facet_wrap(~cargo, scales = \u0026quot;free_y\u0026quot;) + coord_flip() + scale_x_reordered() + scale_fill_manual(values=c(\u0026quot;#4099DA\u0026quot;, \u0026quot;#E85757\u0026quot;)) + labs(title = \u0026quot;Arrival port\u0026quot;, subtitle = \u0026quot;Number of cargoes grouped by arriving from\u0026quot;, y = \u0026quot;\u0026quot;, x = \u0026quot;\u0026quot;) + theme(legend.title = element_blank()) Methodology For solving this supervised classifications task there is different models that can be used and we did try out a logistic regression, KNN and decision tree model. In the end we went with the logistic regression for classification.\nIn the following section we want to work though the essentiel idea behind the method.\nThe logistic regression model Logistic regression is a very popular model for classification problems and is one of the most commonly used Machine Learning algorithms. It is easy to implement and is often used in settings where the dataset is small or as a baseline for any binary classification problem.\nThe method is used when the outcome variable is dichotomous in nature. It is a special case of the general linear regression model when the outcome variable is binary.\nLet \\(y\\) be our binary target with \\(y = 1\\) if the vessel carries WC (the WC class) and \\(y = 0\\) if the cargo is WP (the WP class). Further, let \\(x\\) be a \\(M\\)-vector of features of the observed port calls. Then, given some available information \\(x\\), our interest lies in identifying the probability of a WC vessel and by implication also the probability of a WP vessel. Hence, $$ \\Pr(y = 1 \\mid x) = \\pi^c(x) = 1 - \\Pr(y = 0 \\mid x) = 1 - \\pi^p(x) $$ where \\(\\pi^c(x)\\) and \\(\\pi^p(x)\\) are the conditional probabilities of observing respectively a WC or a WP vessel. The probability mass function for this Bernoulli distribution is $$ p(y \\mid x) = \\pi^c(x) \\cdot \\bigl(1 - \\pi^c(x)\\bigr)^{1-y}. $$\nIn order to proceed from here, one has to make assumptions about the structure of the probability \\(\\pi^c(x)\\). The assumption used in the logistic regression model is to proceed with a logistic function defined by $$ \\sigma(z) = \\frac{1}{1 + \\exp(-z)} = \\frac{\\exp(z)}{\\exp(z) + 1}, $$ with $$ z = \\phi(x)\u0026rsquo; w $$ and finally impose $$ \\pi^c(x) = \\sigma(\\phi(x)\u0026rsquo; w). $$\nThis equation tells that the crux of the logistic regression model is the mapping of \\(z = \\phi(x)' w\\) to a \u0026ldquo;probability\u0026rdquo; through the logistic function.\nThe logistic function gives an S shape curvature. It takes any real vauled number and map it into a value between 0 and 1. Here we have in mind of how the probability should be interpreted. If we get an value 0.75 this mean that there is 75 percent change that a cargo is WC (if WC is baseline).\ncurve({exp(x) / (1 + exp(x))}, -10, 10, xlab = \u0026quot;\u0026quot;, ylab = \u0026quot;\u0026quot;, main = \u0026quot;The logistic function mapping z to the unit interval\u0026quot;) abline(v = 0, lty = \u0026quot;dashed\u0026quot;) For estimated the parameter logistic regression relies on Maximum likelihood estimation (MLE). By this estimation technique the parameter is determined by maximizing the log likelihood function. Here we need to use a optimization algorithm for finding \\(w\\). Often one uses Gradient acent for this purpose.\nThe coefficents should be read as log of odds so we predicts the probability of occurrence of a binary event.\nAnalysis \u0026amp; results For implement the logistic regression we are gonna use the software program R and it machine learning framework tidymodels. tidymodels has its own unique way of structuring a modelling problem and we follows that workflow.\nModel formulation and data preprocessing We split our data set in two sets:\nthe training set: This is the bigger part of the data which we use to train/estimate of our models\nthe test set: This data we hold out to use for evaluation of our trained models\nFurther, we divide our training set into five folds to use for evaluation of the models using cross-validation.1\nset.seed(4595) df \u0026lt;- vessels %\u0026gt;% select(cargo, arriving_from, quay) df_split \u0026lt;- initial_split(df, prop = 0.75, strata = cargo) df_train \u0026lt;- training(df_split) df_test \u0026lt;- testing(df_split) df_train_folds \u0026lt;- vfold_cv(df_train, v = 5, strata = \u0026quot;cargo\u0026quot;) # FIXME Two model specifications are formulated. One where the cargo is predicted using the quay, and one model where the cargo is predicted using the information about the port where the vessel is arriving from.\nfml_1 \u0026lt;- formula(cargo ~ quay) fml_2 \u0026lt;- formula(cargo ~ arriving_from) In the prepoceesion step we gonna use recipe which is part of tidymodels. Here we gonna specify what we want to make of changes to the predictors and handling other issues. In that step we also could implment a method for solving the imbalanced dataset problem, but because it wont give a better result we don´t add this step'.\nlr_1_rec \u0026lt;- recipe(fml_1, data = df_train) %\u0026gt;% step_dummy(all_nominal(), -all_outcomes()) lr_2_rec \u0026lt;- recipe(fml_2, data = df_train) %\u0026gt;% step_dummy(all_nominal(), -all_outcomes()) Logistic regression model training We train a logistic regression model and use five-fold cross-validation to evaluate the model fit. Here we gonna use the recipe from earlier and that and the model to workflow which is handling the information so we easy can make predictions.\nlr_mod \u0026lt;- logistic_reg() %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) lr_wfl_1 \u0026lt;- workflow() %\u0026gt;% add_model(lr_mod) %\u0026gt;% add_recipe(lr_1_rec) lr_wfl_2 \u0026lt;- workflow() %\u0026gt;% add_model(lr_mod) %\u0026gt;% add_recipe(lr_2_rec) lr_wfls \u0026lt;- list(quay = lr_wfl_1, arriving_from = lr_wfl_2) lr_fits_rs \u0026lt;- lr_wfls %\u0026gt;% imap(~ { fit_resamples( .x, resamples = df_train_folds, control = control_resamples(save_pred = TRUE) ) }) ## ## Attaching package: 'rlang' ## The following objects are masked from 'package:purrr': ## ## %@%, as_function, flatten, flatten_chr, flatten_dbl, flatten_int, ## flatten_lgl, flatten_raw, invoke, list_along, modify, prepend, ## splice ## The following object is masked from 'package:magrittr': ## ## set_names ## ## Attaching package: 'vctrs' ## The following object is masked from 'package:tibble': ## ## data_frame ## The following object is masked from 'package:dplyr': ## ## data_frame ## ! Fold1: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici... ## ! Fold2: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici... ## ! Fold3: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici... ## ! Fold4: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici... ## ! Fold5: preprocessor 1/1, model 1/1 (predictions): prediction from a rank-defici... For the evaluation of the model fit we use the model accuracy and the Receiver Operating characteristic (ROC) curve which plots the True Positive Rate (the sensitivity) against the False Positive Rate (1 - specificity/true negative rate). These measures are defined\n$$ \\begin{aligned} \\mathrm{accuracy} \u0026amp;= \\frac{\\mathrm{TP} + \\mathrm{TN}}{N} \\\\ \\text{sensitivity} \u0026amp;= \\frac{\\mathrm{TP}}{\\mathrm{TP} + \\mathrm{FN}} \\\\ \\text{specificity} \u0026amp;= \\frac{\\mathrm{TN}}{\\mathrm{TN} + \\mathrm{FP}} \\end{aligned} $$\nwhere \\(\\mathrm{TP}\\) is number of true WP (true positive), \\(\\mathrm{TN}\\) is the number of true WC (true negative), \\(\\mathrm{FN}\\) is the number of predicted WCs which really were WPs (false negative) and \\(\\mathrm{FP}\\) is the number of falsely predicted WP vessels (false positive).\nThe other measure we are gonna use is the classification accuracy. It is the fraction of predictions our model got right.\nFigure 1 shows the trade-off between TRP and 1-FP. Classifiers that give curves closer to the top-left corner indicate a better performance. Here we see that some gives a perfect prediction.\nlr_fits_rs %\u0026gt;% imap(~ { collect_predictions(.x) %\u0026gt;% group_by(id) %\u0026gt;% roc_curve(truth = cargo, .pred_WP) %\u0026gt;% autoplot() + ggtitle(.y) + theme_orsted(base_family = \u0026quot;\u0026quot;) }) %\u0026gt;% reduce(`+`) Figure 1: ROC curves.\nAs a final clearification we can look at Table 1 that shows us a clear picture of our cross validation and can conclude that we will use quay af a predictor.\nlr_fits_rs %\u0026gt;% imap_dfr(~ relocate(mutate(collect_metrics(.x), model = .y), model)) %\u0026gt;% kbl(caption = \u0026quot;Model accuracy.\u0026quot;) %\u0026gt;% kable_styling() Table 1: Model accuracy. model .metric .estimator mean n std_err .config quay accuracy binary 0.9377778 5 0.0254830 Preprocessor1_Model1 quay roc_auc binary 0.9064931 5 0.0552653 Preprocessor1_Model1 arriving_from accuracy binary 0.8556140 5 0.0102995 Preprocessor1_Model1 arriving_from roc_auc binary 0.7748264 5 0.0681452 Preprocessor1_Model1 Prediction Now that we have trained our model on the training data, we test how well it does on the test data. We are os course certain that quay is the best predictor but we are gonna show the result for both predictors.\n# FIXME prediction from a rank-deficient fit may be misleading. https://stackoverflow.com/a/26560328. lr_fits \u0026lt;- lr_wfls %\u0026gt;% map(~ fit(., data = df_train)) lr_fits %\u0026gt;% imap(~ { predict(.x, df_test, type = \u0026quot;class\u0026quot;) %\u0026gt;% bind_cols(select(df_test, cargo)) %\u0026gt;% conf_mat(truth = cargo, estimate = .pred_class) %\u0026gt;% autoplot(type = \u0026quot;heatmap\u0026quot;) + ggtitle(.y) }) %\u0026gt;% reduce(`+`) ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading lr_fits %\u0026gt;% imap_dfr(~ { predict(.x, df_test, type = \u0026quot;prob\u0026quot;) %\u0026gt;% bind_cols(select(df_test, cargo)) %\u0026gt;% roc_curve(truth = cargo, .pred_WP) %\u0026gt;% mutate(model = .y) }) %\u0026gt;% ggplot(aes(x = 1 - specificity, y = sensitivity, col = model)) + geom_path(lwd = 1.5, alpha = 0.8) + geom_abline(lty = 3) + coord_equal() + scale_color_orsted_d() ## Warning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == : ## prediction from a rank-deficient fit may be misleading lr_pred_prob %\u0026gt;% roc_auc(truth = cargo, .pred_WP) %\u0026gt;% kbl() %\u0026gt;% kable_styling() From the above result we see that the quay as a predictor has correctly predicted every data point in our test set.\nConclusion From our analysis we have created a logistic regression model that predicts if the upcomming vessel is arriving with WP or WC to CMport.\nOur model is using quay as a predictor and this give us good with a very high accuracy.\nEstimation results lr_fits[[1]] %\u0026gt;% pull_workflow_fit() %\u0026gt;% tidy() %\u0026gt;% kbl(digits = 2) %\u0026gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 4.13 1.01 4.10 0.00 quay_c836 -1.56 1.45 -1.08 0.28 quay_c837 -4.41 1.26 -3.49 0.00 quay_c838 -6.61 1.45 -4.56 0.00 lr_fits[[2]] %\u0026gt;% pull_workflow_fit() %\u0026gt;% tidy() %\u0026gt;% kbl(digits = 2) %\u0026gt;% kable_styling() term estimate std.error statistic p.value (Intercept) 20.57 10236.63 0 1 arriving_from_Avedörevaerkets.Havn NA NA NA NA arriving_from_Bandholm..Maribo. 0.00 13541.79 0 1 arriving_from_Bekkeri -41.13 14476.79 0 1 arriving_from_Bremen.Farge NA NA NA NA arriving_from_Cartagena NA NA NA NA arriving_from_Enstedvaerkets.Havn -41.13 16185.54 0 1 arriving_from_Eydehavn 0.00 16185.54 0 1 arriving_from_Gdynia 0.00 13541.79 0 1 arriving_from_Gent 0.00 13541.79 0 1 arriving_from_Greifswald..Landkreis 0.00 13541.79 0 1 arriving_from_Grenå 0.00 20473.27 0 1 arriving_from_Huelva 0.00 20473.27 0 1 arriving_from_Köge NA NA NA NA arriving_from_Larvik -19.87 10236.63 0 1 arriving_from_Leixoes NA NA NA NA arriving_from_Liepaja 0.00 11444.90 0 1 arriving_from_Lübeck 0.00 11356.53 0 1 arriving_from_Lyngdal NA NA NA NA arriving_from_Mandal NA NA NA NA arriving_from_Mersrags -20.57 10236.63 0 1 arriving_from_Papenburg -41.13 20473.27 0 1 arriving_from_Riga -19.55 10236.63 0 1 arriving_from_Rohukuela NA NA NA NA arriving_from_Rostock 0.00 13541.79 0 1 arriving_from_Santana 0.00 16185.54 0 1 arriving_from_Skulte 0.00 16185.54 0 1 arriving_from_Stralsund 0.00 14476.79 0 1 arriving_from_Szczecin NA NA NA NA arriving_from_Szczecinek NA NA NA NA arriving_from_Tallinn -41.13 16185.54 0 1 arriving_from_Törkopp..Drammen. -41.13 20473.27 0 1 arriving_from_Ventspils -19.18 10236.63 0 1 arriving_from_Virtsu 0.00 20473.27 0 1 arriving_from_Wismar 0.00 20473.27 0 1 arriving_from_Övriga.danska.hamnar -41.13 20473.27 0 1 arriving_from_Övriga.estniska.hamnar -41.13 20473.27 0 1 We never use a model\u0026rsquo;s fit to the training set to evaluate the model\u0026rsquo;s performance.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1609977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610010077,"objectID":"8b45e277006a42002e62f138a9f93297","permalink":"/post/cargo-classification/","publishdate":"2021-01-07T00:00:00Z","relpermalink":"/post/cargo-classification/","section":"post","summary":"Introduction Amagerværket has two units 1 and 4 which produces heat and power using respectively wood pellets and wood chips as fuel. The fuel arrives to the plant by vessel and it is possible to track the arrivals through Copenhagen Malmö (CM) Port\u0026rsquo;s website.","tags":["Ørsted","Logistic regression"],"title":"Cargo classification","type":"post"},{"authors":[],"categories":[],"content":" Introduction Trustpilot is a site where user can give a review of different products. The reviews can reveal some essentiel details about the products and service. This can be beneficial for the company to track and analysis the reviews. When there is a lot of reviews it can be hard to keep track of the information. Therefore NPL can be useful in such circumstances.\nAs an example I am gonna use the Nøie reviews to build a topic model to see what is the genreal topics for good and bad reviews.\nWeb scraping For scraping the trutpilot site I am gonna use the browser chrome to look behind the site to look for what data I need to scrape:\nThere is multiple pages of reviews so I am gonna make some general functions to extract the following variables:\nget_ratings\nget_reviews\nget_reviewer_names\nand combine it into a tibble with get_data.\nget_ratings \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;body\u0026quot;) %\u0026gt;% html_nodes(\u0026quot;.star-rating\u0026quot;) %\u0026gt;% as.character() %\u0026gt;% str_subset(\u0026quot;medium\u0026quot;) %\u0026gt;% str_extract(\u0026quot;(\\\\d stjerne)\u0026quot;) %\u0026gt;% str_remove((\u0026quot;( stjerne)\u0026quot;)) %\u0026gt;% unlist() } get_reviews \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;.review-content__body\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_trim() %\u0026gt;% unlist() } get_reviewer_names \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;.consumer-information__name\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_trim() %\u0026gt;% unlist() } get_data \u0026lt;- function(html) { review \u0026lt;- get_reviews(html) names \u0026lt;- get_reviewer_names(html) ratings \u0026lt;- get_ratings(html) data \u0026lt;- tibble( reviewer = names, rating = ratings, review = review ) return(data) } urls \u0026lt;- cbind(c(url1, url2, url3, url4, url5, url6, url7)) url_list \u0026lt;- map(urls, get_data) %\u0026gt;% as.list() data \u0026lt;- do.call(bind_rows, url_list) We see the following information:\nreviewer the user of the product. rating what the reviewer has chosen to give the product on a scale from 1-5. review is the comment given by the reviewer and the central aspect for this analysis. Before we can begin the modelling process we need to preprocess the data. When dealing with unstructred data such as text data the modeller need to use a great amount of time for making the data ready.\nLoading and preparing the data From the data we can see that the reviews are in Danish. Here we can use the happyorsad package to compute a sentiment score for each review. The scores are based on a Danish list of sentiment words and put toheather by Finn Årup Nielsen\nWe are also going to remove numbers, punctuation and stopwords.\ndf \u0026lt;- data %\u0026gt;% mutate(sentiment = map_int(review, happyorsad, \u0026quot;da\u0026quot;)) %\u0026gt;% mutate(review = tolower(review)) %\u0026gt;% mutate( review = removeWords( review, c( \u0026quot;så\u0026quot;, \u0026quot;lidt\u0026quot;, \u0026quot;virkelig\u0026quot;, \u0026quot;virkelig\u0026quot;, \u0026quot;fuldstændig\u0026quot;, \u0026quot;helt\u0026quot;, \u0026quot;mere\u0026quot;, \u0026quot;kan\u0026quot;, \u0026quot;få\u0026quot;, \u0026quot;får\u0026quot;, \u0026quot;fik\u0026quot;, \u0026quot;nøie\u0026quot;, \u0026quot;altså\u0026quot;, \u0026quot;gav\u0026quot;, \u0026quot;endnu\u0026quot;, \u0026quot;sagde\u0026quot;, \u0026quot;ingen\u0026quot;, \u0026quot;flere\u0026quot;, stopwords(\u0026quot;danish\u0026quot;) ) ), review = removeNumbers(review) ) Distribution of sentiment scores In the density plot we see how sentiment scores are distributed with a median score of 8. This a really good score and it is of interst to find out why.\ndf %\u0026gt;% ggplot(aes(x = sentiment)) + geom_density(size = 1) + geom_vline( xintercept = median(df$sentiment), colour = \u0026quot;indianred\u0026quot;, linetype = \u0026quot;dashed\u0026quot;, size = 1 ) + annotate(\u0026quot;text\u0026quot;, x = 15, y = 0.06, label = paste( \u0026quot;median = \u0026quot;, median(df$sentiment) ), colour = \u0026quot;indianred\u0026quot; ) + my_theme() + xlim(-40, 40) In a crude way we can put positive and negative reviews in separate data frames perform topic modelling on each in order to explore what reviewers lik and dislike.\nTopic modelling for positive reviews I start with the positive reviews where I am going to tokenized the data frame which mean one is going to break the text into words so every words can be analyze individually.\ndf_pos \u0026lt;- df %\u0026gt;% filter(sentiment \u0026gt; 1) %\u0026gt;% unnest_tokens(word, review) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremen\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremer\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremejeg\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremene\u0026quot;, \u0026quot;creme\u0026quot;)) Before creating a so called document term matrix we need to count the frequency of each word per document.\nwords_pos \u0026lt;- df_pos %\u0026gt;% count(reviewer, word, sort = TRUE) %\u0026gt;% ungroup() We want to use the famouse Latent Dirichlet Allocation algorithme for Topic modelling. To use this we need to create our DTM and here we use tidytext function cast_dtm to do that.\nreviewDTM_pos \u0026lt;- words_pos %\u0026gt;% cast_dtm(reviewer, word, n) LDA assumes that every document is a mixture of topics, and every topic is a mixture of words. The k argument is used to specify the desired amount of topics that we want in our model. Let´s create a two-topic mode.\nreviewLDA_pos \u0026lt;- LDA(reviewDTM_pos, k = 2, control = list(seed = 123)) The following table shows how many reviews that are assigned to each topic\ntibble(topics(reviewLDA_pos)) %\u0026gt;% group_by(`topics(reviewLDA_pos)`) %\u0026gt;% count() %\u0026gt;% kable(col.names = c(\u0026quot;Positive reviews\u0026quot;, \u0026quot;#\u0026quot;)) %\u0026gt;% kable_styling( full_width = FALSE, position = \u0026quot;left\u0026quot; ) Positive reviews # 1 52 2 57 It is also possible to get the per-topic word probabilities or ‘beta’\ntopics_pos \u0026lt;- tidy(reviewLDA_pos, matrix = \u0026quot;beta\u0026quot;) Now we can find the words with the highest beta. Here we choose the top five words which we will show in a plot.\ntop_terms_pos \u0026lt;- topics_pos %\u0026gt;% group_by(topic) %\u0026gt;% top_n(5, beta) %\u0026gt;% ungroup() %\u0026gt;% arrange(topic, -beta) %\u0026gt;% mutate(order = rev(row_number())) Topic modelling for negative reviews Let us see what can be said regarding the negativ reviews where the sentiment score is below -1\ndf_neg \u0026lt;- df %\u0026gt;% filter(sentiment \u0026lt; -1) %\u0026gt;% unnest_tokens(word, review) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremen\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremer\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremejeg\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremene\u0026quot;, \u0026quot;creme\u0026quot;)) words_neg \u0026lt;- df_neg %\u0026gt;% count(reviewer, word, sort = TRUE) %\u0026gt;% ungroup() reviewDTM_neg \u0026lt;- words_neg %\u0026gt;% cast_dtm(reviewer, word, n) reviewLDA_neg \u0026lt;- LDA(reviewDTM_neg, k = 2, control = list(seed = 347)) tibble(topics(reviewLDA_neg)) %\u0026gt;% group_by(`topics(reviewLDA_neg)`) %\u0026gt;% count() %\u0026gt;% kable(col.names = c(\u0026quot;Negative reviews\u0026quot;, \u0026quot;#\u0026quot;)) %\u0026gt;% kable_styling(full_width = FALSE, position = \u0026quot;left\u0026quot;) Negative reviews # 1 2 2 4 topics_neg \u0026lt;- tidy(reviewLDA_neg, matrix = \u0026quot;beta\u0026quot;) topTerms_neg \u0026lt;- topics_neg %\u0026gt;% group_by(topic) %\u0026gt;% top_n(5, beta) %\u0026gt;% ungroup() %\u0026gt;% arrange(topic, -beta) %\u0026gt;% mutate(order = rev(row_number())) Plotting the topic models Now what the models are on made we can make a plot to make a comparison.\nplot_pos \u0026lt;- top_terms_pos %\u0026gt;% ggplot(aes(order, beta)) + ggtitle(\u0026quot;Positive review topics\u0026quot;) + geom_col(show.legend = FALSE, fill = \u0026quot;steelblue\u0026quot;) + scale_x_continuous( breaks = top_terms_pos$order, labels = top_terms_pos$term, expand = c(0, 0) ) + facet_wrap(~topic, scales = \u0026quot;free\u0026quot;) + coord_flip(ylim = c(0, 0.02)) + my_theme() + theme(axis.title = element_blank()) plot_neg \u0026lt;- topTerms_neg %\u0026gt;% ggplot(aes(order, beta, fill = factor(topic))) + ggtitle(\u0026quot;Negative review topics\u0026quot;) + geom_col(show.legend = FALSE, fill = \u0026quot;indianred\u0026quot;) + scale_x_continuous( breaks = topTerms_neg$order, labels = topTerms_neg$term, expand = c(0, 0) ) + facet_wrap(~topic, scales = \u0026quot;free\u0026quot;) + coord_flip(ylim = c(0, 0.02)) + my_theme() + theme(axis.title = element_blank()) grid.arrange(plot_pos, plot_neg, ncol = 1) So with the above plots we get a feeling on what the reviewer like and dislike regarding Nøie product and brand.\nAs a general notice we are dealing with a really small dataset so this give us some touble sepecially for the negative reviews where there is fewer (a good sign for Nøie).\nLooking for the positive reviews:\nThe creme gives a nice skin and the service is really great. The producs is good and give a super skin. As mention the problem with the nagative reviews is the sparsity of data.\nThe creme gives impurities and zits. Some reviewer think the marketing is bad. Interestingly, customers seem to have both positive and negative experiences with respect to pretty much the same topics. Some customers appear to experience good result from the creme, whereas others seem to complain.\nThis can be explored further.\nWord co-occurrence within reviews To see whether word paris like “bad creame” and “good creme” are frequent in the data sets, we´ll count how many times each pair of words occurs togeather in a title or description field. This can easy be done with pairwise_count() function.\nword_pairs_pos \u0026lt;- df_pos %\u0026gt;% pairwise_count(word, reviewer, sort = TRUE) word_pairs_neg \u0026lt;- df_neg %\u0026gt;% pairwise_count(word, reviewer, sort = TRUE) We can then plot the most common word pairs co-occurring in the reviews by means of the igraph and ggraph packages.\npair_wise_helper \u0026lt;- function(data, title, color) { data %\u0026gt;% graph_from_data_frame() %\u0026gt;% ggraph(layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = color) + ggtitle(title) + geom_node_point(size = 5) + geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \u0026quot;lines\u0026quot;) ) + my_theme() + theme( axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) } pairs_plot_pos \u0026lt;- pair_wise_helper( word_pairs_pos %\u0026gt;% filter(n \u0026gt;= 15), \u0026quot;Positive word pairs\u0026quot;, \u0026quot;steelblue\u0026quot; ) pairs_plot_neg \u0026lt;- pair_wise_helper( word_pairs_neg %\u0026gt;% filter(n \u0026gt; 2), \u0026quot;Negative word pairs\u0026quot;, \u0026quot;indianred\u0026quot; ) grid.arrange(pairs_plot_pos, pairs_plot_neg, ncol = 2) In the positive reviews, it is clear that the word for “creme” tends to co-occur with the word for “hud” and “god”. In the negative reviews, we can se there is not enough data for making any clearness regarding the co-occurance of words.\nWord pair correlations Aanother interesting idea is to measure the correlation for specific words. Here I only look at the positiv dataset because there are to few observation in the negative one.\nAs an alternative idea is to perform an n-gram analysis to find out which words most frequently are used.\ncor_pos \u0026lt;- df_pos %\u0026gt;% group_by(word) %\u0026gt;% filter(n() \u0026gt;= 10) %\u0026gt;% pairwise_cor(word, reviewer, sort = TRUE) %\u0026gt;% filter(item1 == \u0026quot;creme\u0026quot;) %\u0026gt;% top_n(7) %\u0026gt;% mutate( item1 = as.factor(item1), order = rev(row_number()) ) cor_pos %\u0026gt;% ggplot(aes(x = order, y = correlation, fill = item1)) + geom_col(show.legend = FALSE) + scale_x_continuous( breaks = cor_pos$order, labels = cor_pos$item2, expand = c(0, 0) ) + scale_fill_manual(values = c(\u0026quot;steelblue\u0026quot;, \u0026quot;indianred\u0026quot;)) + coord_flip() + labs(x = \u0026quot;words\u0026quot;) + my_theme() The analysis confirm that Nøie has a very high custumer satisfication. The customer are describing the producted to delivered what they want and the service is very satisfiying.\n","date":1609545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609588685,"objectID":"c47731c4b927aa2e763439726df75743","permalink":"/post/test/","publishdate":"2021-01-02T00:00:00Z","relpermalink":"/post/test/","section":"post","summary":"Introduction Trustpilot is a site where user can give a review of different products. The reviews can reveal some essentiel details about the products and service. This can be beneficial for the company to track and analysis the reviews.","tags":[],"title":"Nøie review analysis from Trustpilot","type":"post"},{"authors":null,"categories":null,"content":"In the course \u0026ldquo;Introduction to data science\u0026rdquo; which is about learning essentiel tools for a data scientist including:\nplotting manipulate data regular expressions modelling explanatory data analysis retional database Here we had an contest in finding a data set and show wha you have learn. I my an analysis of gas emission and showed analysis the market and which contries contributed the most to the emission.\n","date":1608768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608768000,"objectID":"50cd8a0c509f10748c1f8cad4ab988e9","permalink":"/project/master_of_the_tidyverse/","publishdate":"2020-12-24T00:00:00Z","relpermalink":"/project/master_of_the_tidyverse/","section":"project","summary":"Made a great repport on gas emissions","tags":["R"],"title":"Master of tidyverse","type":"project"},{"authors":null,"categories":null,"content":"At Ørsted we had a problem with cargoes comming from around the word to Amagerværket. The cargoes could either carry Wood Pellets or Wood Chips. It was every important for the team to know what each cargo were carrying.\nFor solving this problem I found data from scraping the net and made a Machine Learning model that classified each cargo.\n","date":1608768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608768000,"objectID":"ce3bae4521320584bcfed4dd04482beb","permalink":"/project/%C3%B8rsted-classification/","publishdate":"2020-12-24T00:00:00Z","relpermalink":"/project/%C3%B8rsted-classification/","section":"project","summary":"In a collaboration with Ørsted I made a model to classify incoming cargoes","tags":["R","Machine Learning","Ørsted"],"title":"Ørsted Classification model","type":"project"},{"authors":null,"categories":null,"content":"After the financial crisis the world economics saw it self in the deepest recessions since 1939. For comming out of this crisis the Euopean Central Bank (ECB) tried a alternative tool called Quantitative easing. It was implementet as a way to secure price stability which is ECB main agenda. In this analysis I try to quantify weather they made their goal and as a side effect managed to lift the economics. By measuring this I am using a vector autoregressiv (VAR) model.\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"a1a25aab2012fb7a44fd1a1c7a39a64b","permalink":"/project/qe/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/project/qe/","section":"project","summary":"Going thought a VAR model to access the effect of QE","tags":["Economics","R"],"title":"Did QE effect the economics","type":"project"},{"authors":[],"categories":["logistic regression","random forest","tidymodels","parship"],"content":" Introduktion I denne post vil jeg kigger på to modeller. Den logistiske regression og random forest, hvor de begge bliver brugtblog som klassifikations modeller.\nJeg kommer til at gennemgå og beskrive Random Forest da det er en model, som er forholdsvis ny for mig og der er nogle teoretiske framwork jeg gerne vil prøve at forklare.\nDesuden kommer vi til at stifte bekendtskab med parsnip som gør det let at skifte om til de forskellige modeller. Med det nye framwork fra tidymodels kan man skifte utrolig let fra glm til en cross validated random forest med ranger med få linjers koder.\nRandom forest Det er en af de mest populære machine learning algoritmer og kan både bruges som en regresssion og klassifikation model.\nSom navnet antyder så laver algoritmen en skov med forskellige beslutningstræer. Desto flere træer desto mere robust er modellen. Navnet random kommer grundet to koncepter\nEt randomiseret sample af trænings data, når man bygger hver enkelt træ. Et randomiseret subsæt af features, når man splitter noder. Når vi træer hver træ så lærer den fra et random sample af data punkter. Samples er trukket med erstatning, som kaldes bootstrapping, som betyder at et sample vil blive brugt flere gange i et enkelt træ. Ideen er at ved at træne hver træ med forskellige samples, så vil vi få en lavere varians og ikke få et højere bias.\nEns prediction fås ved at tage gennmsnittet af predictor for hver beslutningstræ. Denne procedure kaldes for bagging.\nFordele er man kan bruge den som klassifikation og regression. Den vil ikke overfitte. Den kan håndtere store datasæt med mange dimensioner.\nUlemper er den ikke er så god til regressioner. Den er ikke god til at forudsige. Der er heller ikke meget kontrol over modellen.\nDog er modellen anvendelig i mange sektor såsom banker, forsikringsselskaber, forretninger somkan bruges til at finde de loyolae kunder. Den kan også bruges i aktiemarkedet til ast finde opførelsen af en aktie.\nData I dette projekt bruger jeg data fra Telco Customer Churn. Data indeholder 7043 rækker som hver repræsentere en kunde. Der er 21 kolonner som er mulige predictor, der giver information til vi kan forecast opførelse og give indsigt på forebyggelsesprogrammer.\nChurn er den afhængige variable og viser om kunden har forladt virksomheden indenfor den seneste måned.\nJeg bruger funnktionen skim til at skabe et overblik over mit data.\ntelco \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/treselle-systems/customer_churn_analysis/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) telco %\u0026gt;% skimr::skim() Table 1: Data summary Name Piped data Number of rows 7043 Number of columns 21 _______________________ Column type frequency: character 17 numeric 4 ________________________ Group variables None Variable type: character\nskim_variable n_missing complete_rate min max empty n_unique whitespace customerID 0 1 10 10 0 7043 0 gender 0 1 4 6 0 2 0 Partner 0 1 2 3 0 2 0 Dependents 0 1 2 3 0 2 0 PhoneService 0 1 2 3 0 2 0 MultipleLines 0 1 2 16 0 3 0 InternetService 0 1 2 11 0 3 0 OnlineSecurity 0 1 2 19 0 3 0 OnlineBackup 0 1 2 19 0 3 0 DeviceProtection 0 1 2 19 0 3 0 TechSupport 0 1 2 19 0 3 0 StreamingTV 0 1 2 19 0 3 0 StreamingMovies 0 1 2 19 0 3 0 Contract 0 1 8 14 0 3 0 PaperlessBilling 0 1 2 3 0 2 0 PaymentMethod 0 1 12 25 0 4 0 Churn 0 1 2 3 0 2 0 Variable type: numeric\nskim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist SeniorCitizen 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂ tenure 0 1 32.37 24.56 0.00 9.00 29.00 55.00 72.00 ▇▃▃▃▆ MonthlyCharges 0 1 64.76 30.09 18.25 35.50 70.35 89.85 118.75 ▇▅▆▇▅ TotalCharges 11 1 2283.30 2266.77 18.80 401.45 1397.47 3794.74 8684.80 ▇▂▂▂▁ Her er en række ting at lægge mærke til her.\ncustomerID er en unik id for hver række og af den grund har den ingen deskriptiv eller predictive power og den skal fjernes. Der er meget få NA værdier, så de kan jeg tillade mig at slette. telco \u0026lt;- telco %\u0026gt;% select(-customerID) %\u0026gt;% drop_na() Modellering med tidymodels Denne post giver også en introduktion til tidymodels. Derfor vil modellen være simpel og kommer til at bestå af logistic regression model uden meget data bearbejdring.\nTrain and test split rsample() kan bruges til at lave en randomiserede træning og test data, som selvfølgelig er konstrueret udfra vores orginale telco data.\nset.seed(1972) train_test_split \u0026lt;- rsample::initial_split( data = telco, prop = 0.8 ) train_test_split ## \u0026lt;Analysis/Assess/Total\u0026gt; ## \u0026lt;5626/1406/7032\u0026gt; Ud fra ovenstående har vi at de 7032 kunder er blevet delt ud, og de 5626 er blevet sat i træningssættet. Vi gemmer dem ned i deres eget data frame;\ntrain_tbl \u0026lt;- train_test_split %\u0026gt;% training() %\u0026gt;% unnest() test_tbl \u0026lt;- train_test_split %\u0026gt;% testing() En bage opskrift For at lave en del af arbejde for at bygge modellen bruger vi recipe(). Denne pakke bruger bage metafor til at behandle data og foretage diverse præprocessor såsom, missing values, fjerne predictor, centering og scaling osv..\nDet første man gør er at definere recipe og de transformationer man vil bruge på ens data. Der er ikke meget at gøre i dette tilfælde, udover at tranaformerer til faktor.\nrecipe_simple \u0026lt;- function(dataset) { recipe(Churn ~ ., data = dataset) %\u0026gt;% step_string2factor(all_nominal(), -all_outcomes()) %\u0026gt;% prep(data = dataset) } For at undgår man vi har en data lækage (oveføre information mellem træning og test data), skal data være ‘prepped’ ved kun at bruge train_tbl.\nrecipe_prepped \u0026lt;- recipe_simple(dataset = train_tbl) Som den sidste del så skal vi bage opskriften for at alle præprocessor bliver inkluderet i data sættene.\ntrain_baked \u0026lt;- bake(recipe_prepped, new_data = train_tbl) test_baked \u0026lt;- bake(recipe_prepped, new_data = test_tbl) Fit modellen Tidymodels er det helt nye indspark fra tidyverse folkene på at skabe et framwork for machine learning. Hertil er der blevet lavet en del justeringer og nye pakker. En central pakke i dette framwork er parsnip,som skaber en adgang til mange machine learning pakker uden man skal kunne syntaksen til dem alle.\nMan skal følge tre trin:\nBestem typen af modellen og mode. Bestem engine. Bestem model specifikationer og data der skal bruges. logistic_glm \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% fit(Churn ~ ., data = train_baked) Som sagt så kan du vælge en masse andre engine. I dette tilfælde hvor vi bruge en logistisk regression, så kan vi vælge; glm, glmnet, stan, spark og keras. Det smarte er vi bare kan skifte det ud og så klare parsnip transitionen.\nHvor godt klare modellen sig? Det er væsentlig at se hvor god modellen er og her bruger vi pakken yardstick, som gør det let at beregne forskellige måleværktøjer. Før man kan beregne disse måle enheder skal vi beregne nogle predictor ved at bruge test_baked til predict funktionen.\nprediction_glm \u0026lt;- logistic_glm %\u0026gt;% predict(new_data = test_baked) %\u0026gt;% bind_cols(test_baked %\u0026gt;% select(Churn)) head(prediction_glm) ## # A tibble: 6 x 2 ## .pred_class Churn ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Yes No ## 2 No No ## 3 No No ## 4 No No ## 5 No No ## 6 No No Der kan benyttes mange matricer til at undersøge hvor god modellen er, men fokus for denne post bliver accuracy, precision, recall og F1_score.\nDisse mål bliver udledt af Confusion Matrix, som er en tabel der beskriver hvor godt ens klassifikations model klarer sig. Denne matrice er i sig selv ikke svær at forstå, da den angiver antallet af; false positives, false negatives, true positives og true negatives. Dog er nogle af målene, som udledes herfra svære koncepter og kræver reflektion for at forstå deres betydning.\nprediction_glm %\u0026gt;% conf_mat(Churn, .pred_class) %\u0026gt;% pluck(1) %\u0026gt;% as_tibble() %\u0026gt;% ggplot(aes(Prediction, Truth, alpha = n)) + geom_tile(show.legend = FALSE) + geom_text(aes(label = n), colour = \u0026quot;white\u0026quot;, alpha = 0.5, size = 12) Modellen Accuracy er andel af prediction modellen ramte plet og kan udregnes ved at lade predictions_glm gå gennem metrics funktionen. Dog er den ikke så troværdig, hvis ens data er ubalanceret.\nprediction_glm %\u0026gt;% metrics(Churn, .pred_class) %\u0026gt;% select(-.estimator) %\u0026gt;% filter(.metric == \u0026quot;accuracy\u0026quot;) ## # A tibble: 1 x 2 ## .metric .estimate ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy 0.806 Modellen får altså en score på 78%.\nPrecision målser hvor sensitiv modellen er overfor False Positive, mens Recall ser hvor sensitiv modellen er for False Negative.\nDisse metricer er meget vigtig informationer for virksomheder fordi man så kan forudsige hvilke kunder der er i en risiko gruppe for at forlade forretningen. Herfra kan man så benytte sig af en fastholdessstrategi. Desuen kan man bruge oplysning til ikke at bruge penge på kudner der alligevel har tænkt sig at forlade virksomheden.\ntibble( \u0026quot;precision\u0026quot; = precision(prediction_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), \u0026quot;recall\u0026quot; = recall(prediction_glm, Churn, .pred_class) %\u0026gt;% select(.estimate) ) %\u0026gt;% unnest() %\u0026gt;% kable() precision recall 0.8466368 0.9024857 Den anden og sidste populær måleværktøj er F1_score, som er det harmoniske gennemsnit af precision og recall. Den perfekte score på 1 fås når precision og recall er perfekte.\nprediction_glm %\u0026gt;% f_meas(Churn, .pred_class) %\u0026gt;% select(-.estimator) %\u0026gt;% kable() .metric .estimate f_meas 0.8736696 Fra logitstik regression til Random Forest Det er utrolig simpel at skifte ens model ud med en anden. Den tidligere anvendte logistisk regressions model kan vi hurtig skifte ud med en Random Forest model med ranger.\nCroos validation sæt op For at styke modellens prediktive kræft kan man foretage cross validation, som tit bliver sat op med 10 folder. Det kan implementeres med vfold_cv() fra rsample, som splitter det initale trænings data.\nset.seed(123) cross_val_tbl \u0026lt;- vfold_cv(train_tbl, v = 10) Vi kan genkende de 5626 fra vores tærningssæt. I hver runde vil 563 observationer blive brugt til validere modellen for det specifikke fold.\nFor at ikke blive forvirret over bruget af initial træsning/test split til det man bruger i cross validation benytter man begreberne analysis (estimer modellen) og assessment (valider estimater).\nOpdater recipe For at bruge Random Forest skal alle numeriske værdier være centred og scaled og alle faktor skal være dummies.\nsplit \u0026lt;- initial_split(telco, prop = 0.8) train_data \u0026lt;- training(split) test_data \u0026lt;- testing(split) For at skifte over til en anden model er utroligt simepel. Her ændre vi til random forest i typen af modellen og tilføjer dens hyperparameter.\nFor at gøre processen lidt hurtigere propper jeg det hele i en funktion, som estimer modellen på tværs af alle folder og retuner det i en tibble. Desuden skal der tilføjes et skridt mere for at vi mapper de forskellige folder.\nrecipe_rf \u0026lt;- function(dataset) { recipe(Churn ~ ., data = dataset) %\u0026gt;% step_string2factor(all_nominal(), -all_outcomes()) %\u0026gt;% step_dummy(all_nominal(), -all_outcomes()) %\u0026gt;% step_center(all_numeric()) %\u0026gt;% step_scale(all_numeric()) %\u0026gt;% prep(data = dataset) } rf_fun \u0026lt;- function(split, id, try, tree) { analysis_set \u0026lt;- split %\u0026gt;% analysis() analysis_prepped \u0026lt;- analysis_set %\u0026gt;% recipe_rf() analysis_baked \u0026lt;- analysis_prepped %\u0026gt;% bake(new_data = analysis_set) model_rf \u0026lt;- rand_forest( mode = \u0026quot;classification\u0026quot;, mtry = try, trees = tree ) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;, importance = \u0026quot;impurity\u0026quot; ) %\u0026gt;% fit(Churn ~ ., data = analysis_baked) assessment_set \u0026lt;- split %\u0026gt;% assessment() assessment_prepped \u0026lt;- assessment_set %\u0026gt;% recipe_rf() assessment_baked \u0026lt;- assessment_prepped %\u0026gt;% bake(new_data = assessment_set) tibble( \u0026quot;id\u0026quot; = id, \u0026quot;truth\u0026quot; = assessment_baked$Churn, \u0026quot;prediction\u0026quot; = model_rf %\u0026gt;% predict(new_data = assessment_baked) %\u0026gt;% unlist() ) } pred_rf \u0026lt;- map2_df( .x = cross_val_tbl$splits, .y = cross_val_tbl$id, ~ rf_fun(split = .x, id = .y, try = 3, tree = 200) ) head(pred_rf) ## # A tibble: 6 x 3 ## id truth prediction ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Fold01 Yes No ## 2 Fold01 Yes Yes ## 3 Fold01 No No ## 4 Fold01 No No ## 5 Fold01 No No ## 6 Fold01 No No pred_rf %\u0026gt;% conf_mat(truth, prediction) %\u0026gt;% summary() %\u0026gt;% select(-.estimator) %\u0026gt;% filter(.metric %in% c(\u0026quot;accuracy\u0026quot;, \u0026quot;precision\u0026quot;, \u0026quot;recall\u0026quot;, \u0026quot;f_meas\u0026quot;)) %\u0026gt;% kable() .metric .estimate accuracy 0.7996801 precision 0.8291502 recall 0.9147437 f_meas 0.8698464 Der er mange matricer til at validere vores model, men vi bruger dem som vi brugte ved vores logistisk regression.\nModellen klare sig på lige fod med regressionsmodellen. Man kunne gå tilbage til modellen og laver yderligere feature eengierning da det ville gøre noget for selve præcisionen af modellen.\n","date":1581897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589739594,"objectID":"2dd1ed83472053f5e0c15ff91faaa973","permalink":"/post/tidymodels-and-parsnip/","publishdate":"2020-02-17T00:00:00Z","relpermalink":"/post/tidymodels-and-parsnip/","section":"post","summary":"Introduktion I denne post vil jeg kigger på to modeller. Den logistiske regression og random forest, hvor de begge bliver brugtblog som klassifikations modeller.\nJeg kommer til at gennemgå og beskrive Random Forest da det er en model, som er forholdsvis ny for mig og der er nogle teoretiske framwork jeg gerne vil prøve at forklare.","tags":["tidymodels","ML","parship","random forest"],"title":"Tidymodels, Random Forest og parsnip","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation Features Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export : E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask Documentation ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"077e1e01fd8590c86aff39607410dbcb","permalink":"/project_landing/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project_landing/","section":"","summary":"Hello!","tags":null,"title":"Projects","type":"widget_page"}]