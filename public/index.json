[{"authors":["admin"],"categories":null,"content":"I am a passionate data scientist with key interest in how to apply data science toolbox in a business and public manner.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/","section":"authors","summary":"I am a passionate data scientist with key interest in how to apply data science toolbox in a business and public manner.","tags":null,"title":"Lucas Bagge","type":"authors"},{"authors":[],"categories":["text mining"],"content":"Introduction In this analysis I am gonna scapes truspilot web page for reviews given by customer for the skincare firm Nøie.\nHere I am gonna use the data to make some topic modelling.\nWeb scraping For sciping the trutpilot site I am gonna make three functions: get_ratings, get_reviews and get_reviewer_names and combine it into a tibble with get_data to extract the data.\nget_ratings \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;body\u0026quot;) %\u0026gt;% html_nodes(\u0026quot;.star-rating\u0026quot;) %\u0026gt;% as.character() %\u0026gt;% str_subset(\u0026quot;medium\u0026quot;) %\u0026gt;% str_extract(\u0026quot;(\\\\d stjerne)\u0026quot;) %\u0026gt;% str_remove((\u0026quot;( stjerne)\u0026quot;)) %\u0026gt;% unlist() } get_reviews \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;.review-content__body\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_trim() %\u0026gt;% unlist() } get_reviewer_names \u0026lt;- function(html) { html %\u0026gt;% read_html() %\u0026gt;% html_nodes(\u0026quot;.consumer-information__name\u0026quot;) %\u0026gt;% html_text() %\u0026gt;% str_trim() %\u0026gt;% unlist() } get_data \u0026lt;- function(html) { review \u0026lt;- get_reviews(html) names \u0026lt;- get_reviewer_names(html) ratings \u0026lt;- get_ratings(html) data \u0026lt;- tibble( reviewer = names, rating = ratings, review = review ) data } urls \u0026lt;- cbind(c(url1, url2, url3, url4, url5, url6, url7)) url_list \u0026lt;- map(urls, get_data) %\u0026gt;% as.list() data \u0026lt;- do.call(bind_rows, url_list)  Let us quick take a look at what I have extracted from the site:\ndata %\u0026gt;% head()  ## # A tibble: 6 x 3 ## reviewer rating review ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 Lars Jensen 5 \u0026quot;Gode produkter og super service. Skru ned for… ## 2 Maria Cirkeline Rasmin… 5 \u0026quot;Super produkter\\n \\n \\n… ## 3 Lis 1 \u0026quot;2 cremer gav endnu mere uren hud + ulovlig ma… ## 4 Trine Holm 5 \u0026quot;Min hud slår altid ud om vinteren.\\n … ## 5 Camilla 1 \u0026quot;Min hud er værre end nogensinde\\n … ## 6 Andrea Broe 5 \u0026quot;Stor anbefaling\\n \\n \\n…  We see the following information:\n reviewer that is the person that has used the product. rating what the the reviewer has chosen to give the product on a scale from 1-5. review is the comment given by the reviewer and the central aspect for this analysis.  In the next section I will drewll into into the preprocessig step for this text mining task.\nLoading and preparing the data From the data we can see that the reviews are in Danish. Here we can use the happyorsad package to compute a sentiment score for each review. Thease score are based on a Danish list of sentiment words and put toheather by Finn Årup Nielsen df \u0026lt;- data %\u0026gt;% mutate(sentiment = map_int(review, happyorsad, \u0026quot;da\u0026quot;)) %\u0026gt;% mutate(review = tolower(review)) %\u0026gt;% mutate(review = removeWords( review, c( \u0026quot;så\u0026quot;, \u0026quot;3\u0026quot;, \u0026quot;kan\u0026quot;, \u0026quot;få\u0026quot;, \u0026quot;får\u0026quot;, \u0026quot;fik\u0026quot;, \u0026quot;nøie\u0026quot;, stopwords(\u0026quot;danish\u0026quot;) ) ))  Distribution of sentiment scores In the density plot we see how sentiment scores are distributed with a median score of 2. This a really good score and it is of interst to find out why Nøie has a this great score and it also svore 4.5 rating out of 5.\ndf %\u0026gt;% ggplot(aes(x = sentiment)) + geom_density(size = 1) + geom_vline( xintercept = median(df$sentiment), colour = \u0026quot;indianred\u0026quot;, linetype = \u0026quot;dashed\u0026quot;, size = 1 ) + ggplot2::annotate(\u0026quot;text\u0026quot;, x = 15, y = 0.06, label = paste(\u0026quot;median = \u0026quot;, median(df$sentiment)), colour = \u0026quot;indianred\u0026quot; ) + my_theme() + xlim(-40, 40)  In a crude way we can put positive and negative reviews in separate data frames perform topic modelling on each in order to explore what reviewers lik and dislike.\nTopic modelling for positive reviews df_pos \u0026lt;- df %\u0026gt;% filter(sentiment \u0026gt; 1) %\u0026gt;% unnest_tokens(word, review) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremen\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremer\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremejeg\u0026quot;, \u0026quot;creme\u0026quot;)) %\u0026gt;% mutate(word = str_replace(word, \u0026quot;cremene\u0026quot;, \u0026quot;creme\u0026quot;))  Before creating a so called document term matrix we need to count the frequency of each word per document.\nwords_pos \u0026lt;- df_pos %\u0026gt;% count(reviewer, word, sort = TRUE) %\u0026gt;% ungroup()  We want to use the famouse Latent Dirichlet Allocation algorithme for topic modelling. To use this we need to create our DTM and here we use tidytext function cast_dtm to do that.\nreviewDTM_pos \u0026lt;- words_pos %\u0026gt;% cast_dtm(reviewer, word, n)  LDA assumes that every document is a mixture of topics, and every topic is a mixture of words. The k argument is used to specify the desired amount of topics that we want in our model. Let´s create a two-topic mode.\nreviewLDA_pos \u0026lt;- LDA(reviewDTM_pos, k = 3, control = list(seed = 123))  The following table shows how many reviews that are assigned to each topic\ntibble(topics(reviewLDA_pos)) %\u0026gt;% group_by(`topics(reviewLDA_pos)`) %\u0026gt;% count() %\u0026gt;% kable() %\u0026gt;% kable_styling( full_width = FALSE, position = \u0026quot;left\u0026quot; )    topics(reviewLDA_pos)  n      1  50    2  29    3  30     It is also possible to get the per-topic word probabilities or \u0026lsquo;beta\u0026rsquo;\ntopics_pos \u0026lt;- tidy(reviewLDA_pos, matrix = \u0026quot;beta\u0026quot;) topics_pos  ## # A tibble: 3,336 x 3 ## topic term beta ## \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 creme 0.0611 ## 2 2 creme 0.0320 ## 3 3 creme 0.0499 ## 4 1 tak 0.000964 ## 5 2 tak 0.00569 ## 6 3 tak 0.00600 ## 7 1 hud 0.0216 ## 8 2 hud 0.0229 ## 9 3 hud 0.0275 ## 10 1 produkter 0.0229 ## # … with 3,326 more rows  Now we can find the words with the highest beta. Here we choose the top five words which we will show in a plot.\ntop_terms_pos \u0026lt;- topics_pos %\u0026gt;% group_by(topic) %\u0026gt;% top_n(5, beta) %\u0026gt;% ungroup() %\u0026gt;% arrange(topic, -beta) %\u0026gt;% mutate(order = rev(row_number()))  # plot_pos \u0026lt;- top_terms_pos %\u0026gt;% ggplot(aes(order, beta)) + ggtitle(\u0026quot;Positive review topics\u0026quot;) + geom_col(show.legend = FALSE, fill = \u0026quot;steelblue\u0026quot;) + scale_x_continuous( breaks = top_terms_pos$order, labels = top_terms_pos$term, expand = c(0, 0) ) + facet_wrap(~topic, scales = \u0026quot;free\u0026quot;) + coord_flip(ylim = c(0, 0.02)) + my_theme() + theme(axis.title = element_blank())  Word co-occurrence within reviews pairs_plot_pos \u0026lt;- word_pairs_pos \u0026lt;- df_pos %\u0026gt;% pairwise_count(word, reviewer, sort = TRUE) %\u0026gt;% filter(n \u0026gt;= 10) %\u0026gt;% graph_from_data_frame() %\u0026gt;% ggraph(layout = \u0026quot;fr\u0026quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \u0026quot;steelblue\u0026quot;) + ggtitle(\u0026quot;Positive word pairs\u0026quot;) + geom_node_point(size = 5) + geom_node_text(aes(label = name), repel = TRUE, point.padding = unit(0.2, \u0026quot;lines\u0026quot;) ) + my_theme() + theme( axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank() ) grid.arrange(pairs_plot_pos)  ","date":1609200000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609278597,"objectID":"eb425cf96d9cdd3fabb59936e68a4a9b","permalink":"/post/text-mining-n%C3%B8ie-trust-pilot-reviews/","publishdate":"2020-12-29T00:00:00Z","relpermalink":"/post/text-mining-n%C3%B8ie-trust-pilot-reviews/","section":"post","summary":"Introduction In this analysis I am gonna scapes truspilot web page for reviews given by customer for the skincare firm Nøie.\nHere I am gonna use the data to make some topic modelling.","tags":["tidytext","topic models"],"title":"Text mining - nøie trust pilot reviews","type":"post"},{"authors":null,"categories":null,"content":"In the course \u0026ldquo;Introduction to data science\u0026rdquo; which is about learning essentiel tools for a data scientist including:\n plotting manipulate data regular expressions modelling explanatory data analysis retional database  Here we had an contest in finding a data set and show wha you have learn. I my an analysis of gas emission and showed analysis the market and which contries contributed the most to the emission.\n","date":1608768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608768000,"objectID":"50cd8a0c509f10748c1f8cad4ab988e9","permalink":"/project/master_of_the_tidyverse/","publishdate":"2020-12-24T00:00:00Z","relpermalink":"/project/master_of_the_tidyverse/","section":"project","summary":"Made a great repport on gas emissions","tags":["R"],"title":"Master of tidyverse","type":"project"},{"authors":[],"categories":[],"content":"  RStudio has recently released a cohesive suite of packages for modelling and machine learning, called {tidymodels}. The successor to Max Kuhn’s {caret} package, {tidymodels} allows for a tidy approach to your data from start to finish. We’re going to walk through the basics for getting off the ground with {tidymodels} and demonstrate its application to three different tree-based methods for predicting student test scores. For further information about the package, you can visit https://www.tidymodels.org/.\nSetup Load both the {tidyverse} and {tidymodels} packages into your environment. We’ll also load in the {skimr} package to help us with some descriptives for our data and a host of other packages that will be required to run our machine learning models.\nlibrary(tidymodels) library(tidyverse) # manipulating data library(skimr) # data visualization library(baguette) # bagged trees library(future) # parallel processing \u0026amp; decrease computation time library(xgboost) # boosted trees  Import the data We use simulated data which approximates reading and math scores for ~189,000 3rd-8th grade students in Oregon public schools see this Kaggle page for details. For the purpose of demonstration, we’ll be sampling 1% of the data with sample_frac() to keep computer processing time manageable. All school IDs in the data are real, so we can use that information to link the data with other sources. Specifically, we’re also going to pull in some data on student enrollment in free and reduced lunch from the National Center for Education Statistics and some ethnicity data from the Oregon Department of Education.\nset.seed(100) # import data and perform initial cleaning # initial cleaning steps include: # *recode NA\u0026#39;s for lang_cd and ayp_lep to more meaningful values # *remove vars with entirely missing data # Note: the data is called \u0026#39;train.csv\u0026#39;, but we will actually further split this into its own training and testing data dat \u0026lt;- read_csv(here::here(\u0026quot;static\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;train.csv\u0026quot;)) %\u0026gt;% select(-classification) %\u0026gt;% # remove this variable because it\u0026#39;s redundant with `score` mutate(lang_cd = ifelse(is.na(lang_cd), \u0026quot;E\u0026quot;, lang_cd), ayp_lep = ifelse(is.na(ayp_lep), \u0026quot;G\u0026quot;, ayp_lep)) %\u0026gt;% sample_frac(.01) %\u0026gt;% # sample 1% of the data to reduce run time janitor::remove_empty(c(\u0026quot;rows\u0026quot;, \u0026quot;cols\u0026quot;)) %\u0026gt;% drop_na() %\u0026gt;% select_if(~length(unique(.x)) \u0026gt; 1) # import fall membership report ethcnicity data and do some basic cleaning and renaming sheets \u0026lt;- readxl::excel_sheets(here::here(\u0026quot;static\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;fallmembershipreport_20192020.xlsx\u0026quot;)) ode_schools \u0026lt;- readxl::read_xlsx(here::here(\u0026quot;static\u0026quot;, \u0026quot;data\u0026quot;, \u0026quot;fallmembershipreport_20192020.xlsx\u0026quot;), sheet = sheets[4]) ethnicities \u0026lt;- ode_schools %\u0026gt;% select(attnd_schl_inst_id = `Attending School ID`, attnd_dist_inst_id = `Attending District Institution ID`, sch_name = `School Name`, contains(\u0026quot;%\u0026quot;)) %\u0026gt;% janitor::clean_names() names(ethnicities) \u0026lt;- gsub(\u0026quot;x2019_20_percent\u0026quot;, \u0026quot;p\u0026quot;, names(ethnicities)) # join ethnicity data with original dataset dat \u0026lt;- left_join(dat, ethnicities) # import and tidy free and reduced lunch data frl \u0026lt;- rio::import(\u0026quot;https://nces.ed.gov/ccd/Data/zip/ccd_sch_033_1718_l_1a_083118.zip\u0026quot;, setclass = \u0026quot;tbl_df\u0026quot;) %\u0026gt;% janitor::clean_names() %\u0026gt;% filter(st == \u0026quot;OR\u0026quot;) %\u0026gt;% select(ncessch, lunch_program, student_count) %\u0026gt;% mutate(student_count = replace_na(student_count, 0)) %\u0026gt;% pivot_wider(names_from = lunch_program, values_from = student_count) %\u0026gt;% janitor::clean_names() %\u0026gt;% mutate(ncessch = as.double(ncessch)) # import student counts for each school across grades stu_counts \u0026lt;- rio::import(\u0026quot;https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv\u0026quot;, setclass = \u0026quot;tbl_df\u0026quot;) %\u0026gt;% filter(state == \u0026quot;OR\u0026quot; \u0026amp; year == 1718) %\u0026gt;% count(ncessch, wt = n) %\u0026gt;% mutate(ncessch = as.double(ncessch)) # join frl and stu_counts data frl \u0026lt;- left_join(frl, stu_counts) # add frl data to train data dat \u0026lt;- left_join(dat, frl) After loading in our three datasets, we’ll join them together to make one cohesive data set to use for modelling. After joining, the data contains both student-level variables (e.g. gender, ethnicity, enrollment in special education/talented and gifted programs, etc.) and district-level variables (e.g. school longitude and latitude, proportion of students who qualify for free and reduced-price lunch, etc.), all of which will be included for each 3 of our {tidymodels} tree-based examples.\nFor a more complete description of the variables, you can download the data dictionary here.\n Explore the data We’ll use the skim() function from {skimr} to take a closer look at our variables. Many numeric predictors are clearly non-normal (see histograms below), but this is no problem as tree-based methods are robust to non-normality.\ndat %\u0026gt;% select(-contains(\u0026quot;id\u0026quot;), -ncessch, -missing, -not_applicable) %\u0026gt;% # remove ID and irrelevant variables mutate(tst_dt = lubridate::as_date(lubridate::mdy_hms(tst_dt))) %\u0026gt;% # covert test date to date modify_if(is.character, as.factor) %\u0026gt;% # convert character vars to factors skim() %\u0026gt;% select(-starts_with(\u0026quot;numeric.p\u0026quot;)) # remove quartiles  Table 1: Data summary  Name Piped data  Number of rows 1857  Number of columns 41  _______________________   Column type frequency:   Date 1  factor 25  numeric 15  ________________________   Group variables None    Variable type: Date\n  skim_variable n_missing complete_rate min max median n_unique    tst_dt 0 1 2018-03-16 2018-06-07 2018-05-18 47    Variable type: factor\n  skim_variable n_missing complete_rate ordered n_unique top_counts    gndr 0 1 FALSE 2 M: 939, F: 918  ethnic_cd 0 1 FALSE 7 W: 1151, H: 458, M: 100, A: 79  tst_bnch 0 1 FALSE 6 G6: 343, 1B: 330, G4: 304, G7: 304  migrant_ed_fg 0 1 FALSE 2 N: 1793, Y: 64  ind_ed_fg 0 1 FALSE 2 N: 1842, Y: 15  sp_ed_fg 0 1 FALSE 2 N: 1614, Y: 243  tag_ed_fg 0 1 FALSE 2 N: 1759, Y: 98  econ_dsvntg 0 1 FALSE 2 Y: 1100, N: 757  ayp_lep 0 1 FALSE 10 G: 1471, F: 164, Y: 72, E: 58  stay_in_dist 0 1 FALSE 2 Y: 1811, N: 46  stay_in_schl 0 1 FALSE 2 Y: 1803, N: 54  dist_sped 0 1 FALSE 2 N: 1846, Y: 11  trgt_assist_fg 0 1 FALSE 3 N: 1773, Y: 83, y: 1  ayp_schl_partic 0 1 FALSE 2 Y: 1846, N: 11  ayp_dist_prfrm 0 1 FALSE 2 Y: 1803, N: 54  ayp_schl_prfrm 0 1 FALSE 2 Y: 1785, N: 72  rc_schl_partic 0 1 FALSE 2 Y: 1846, N: 11  rc_dist_prfrm 0 1 FALSE 2 Y: 1803, N: 54  rc_schl_prfrm 0 1 FALSE 2 Y: 1785, N: 72  lang_cd 0 1 FALSE 2 E: 1815, S: 42  tst_atmpt_fg 0 1 FALSE 2 Y: 1853, P: 4  grp_rpt_schl_partic 0 1 FALSE 2 Y: 1846, N: 11  grp_rpt_dist_prfrm 0 1 FALSE 2 Y: 1845, N: 12  grp_rpt_schl_prfrm 0 1 FALSE 2 Y: 1834, N: 23  sch_name 1 1 FALSE 699 Hig: 14, Jud: 14, Hou: 13, Fiv: 11    Variable type: numeric\n  skim_variable n_missing complete_rate mean sd hist    enrl_grd 0 1 5.44 1.69 ▇▃▅▃▃  score 0 1 2495.34 115.19 ▁▁▂▇▁  lat 0 1 44.79 0.99 ▂▁▂▅▇  lon 0 1 -122.51 1.16 ▅▇▁▁▁  p_american_indian_alaska_native 1 1 0.01 0.06 ▇▁▁▁▁  p_asian 1 1 0.04 0.07 ▇▁▁▁▁  p_native_hawaiian_pacific_islander 1 1 0.01 0.01 ▇▁▁▁▁  p_black_african_american 1 1 0.02 0.04 ▇▁▁▁▁  p_hispanic_latino 1 1 0.25 0.18 ▇▅▂▁▁  p_white 1 1 0.60 0.20 ▁▃▅▇▅  p_multiracial 1 1 0.06 0.03 ▇▆▁▁▁  free_lunch_qualified 0 1 231.23 147.55 ▇▇▃▁▁  reduced_price_lunch_qualified 0 1 39.86 24.77 ▆▇▃▁▁  no_category_codes 0 1 271.09 165.44 ▆▇▃▁▁  n 0 1 816.07 536.55 ▇▃▂▁▁    While most of our predictors are categorical, we can use {corrplot} to better visualize the relationships among the numeric variables.\ndat %\u0026gt;% select(-contains(\u0026quot;id\u0026quot;), -ncessch, -missing, -not_applicable) %\u0026gt;% select_if(is.numeric) %\u0026gt;% select(score, everything()) %\u0026gt;% cor(use = \u0026quot;complete.obs\u0026quot;) %\u0026gt;% corrplot::corrplot()  Split data and resample The first step of our analysis is to split our data into two separate sets: a “training” set and a “testing” set. The training set is used to train a model and, if desired, to adjust (i.e., “tune”) the model’s hyperparameters before evaluating its final performance on our test data. By allowing us to test a model on a new sample, we assess “out of sample” accuracy (i.e., unseen data-—what all predictive models are interested in) and limit overfitting to the training set. We can do this efficiently with the initial_split() function. This comes from the {rsample} package, which is part of the {tidymodels} package that we already loaded. Defaults put 75% of the data in the training set and 25% in the test set, but this can be adjusted with the prop argument. Then, we’ll extract the training data from our split object and assign it a name.\nTo further prevent over-fitting, we’ll resample our data using vfold_cv(). This function outputs k-fold cross-validated versions of our training data, where k = the number of times we resample (unsure why v- is used instead of k- here). By using k = 10 data sets, we get a better estimate of the model’s out-of-sample accuracy. On top of decreasing bias from over-fitting, this is essential when tuning hyperparameters (though we plan to apply defaults and not tune here, for brevity). Though our use of 10-fold cross validation is both frequently used and effective, it should be noted that other methods (e.g., bootstrap resampling) or other k-values are sometimes used to accomplish the same goal.\n# split the data split \u0026lt;- initial_split(dat) # extract the training data train \u0026lt;- training(split) # resample the data with 10-fold cross-validation (10-fold by default) cv \u0026lt;- vfold_cv(train)  Pre-processing Before we add in our data to the model, we’re going to set up an object that pre-processes our data. This is called a recipe. To create a recipe, you’ll first specify a formula for your model, indicating which variable is your outcome and which are your predictors. Using ~. here will indicate that we want to use all variables other than score as predictors. Then, we can specify a series of pre-processing steps for our data that directs our recipe to assign our variables a role or performs feature engineering steps. Pre-processing may be sound uncommon, but if you’ve ever used lm() (or several other R functions) you’ve done some of this by simply calling the function (e.g., automatic dummy-coding to handle categorical data). This is beneficial because it gives the analyst more control, despite adding complexity to the process.\nA complete list of possible pre-processing steps can be found here: https://www.tidymodels.org/find/recipes/\nrec \u0026lt;- recipe(score ~ ., train) %\u0026gt;% step_mutate(tst_dt = as.numeric(lubridate::mdy_hms(tst_dt))) %\u0026gt;% # convert `test date` variable to a date update_role(contains(\u0026quot;id\u0026quot;), ncessch, new_role = \u0026quot;id vars\u0026quot;) %\u0026gt;% # declare ID variables step_nzv(all_predictors(), freq_cut = 0, unique_cut = 0) %\u0026gt;% # remove variables with zero variances step_novel(all_nominal()) %\u0026gt;% # prepares test data to handle previously unseen factor levels step_unknown(all_nominal()) %\u0026gt;% # categorizes missing categorical data (NA\u0026#39;s) as `unknown` step_medianimpute(all_numeric(), -all_outcomes(), -has_role(\u0026quot;id vars\u0026quot;)) %\u0026gt;% # replaces missing numeric observations with the median step_dummy(all_nominal(), -has_role(\u0026quot;id vars\u0026quot;)) # dummy codes categorical variables  Create a model The last step before bringing in our data is to specify our model. This will call upon functions from the {parsnip} package, which standardizes language for specifying a multitude of statistical models. There are a few core elements that you will need to specify for each model\nThe type of model This indicates what type of model you choose to fit, each of which will be a different function. We’ll be focusing on decision tree methods using bag_tree(), random_forest(), and boost_tree(). A full list of models can be found here https://www.tidymodels.org/find/parsnip/\n The engine set_engine() calls the package to support the model you specified above.\n The mode set_mode() indicates the type of prediction you’d like to use in your model, you’ll choose between regression and classification. Since we are looking to predict student scores, which is a continuous predictor, we’ll be choosing regression.\n The arguments set_args() allows you to set values for various parameters for your model, each model type will have a specific set of parameters that can be altered. For these parameters, you can either set a particular value or you can use the tune function to search for the optimal value of each parameter. Tuning requires a few extra steps, so we will leave the default arguments for clarity. For more information on tuning check out https://tune.tidymodels.org/.\n  Create a workflow Up to this point we’ve been setting up a lot of individual elements and now it is time to combine them to create a cohesive framework, called a workflow, so we can run our desired models. First, we’ll use the workflow() command and then we’ll pulling the recipe and model we already created. The next section shows three examples of specifying models and creating a workflow for different decision tree methods.\n Model Examples Bagged trees A bagged tree approach creates multiple subsets of data from the training set which are randomly chosen with replacement. Each subset of data is used to train a given decision tree. In the end, we have an ensemble of different models. The predictions from all the different trees are averaged together, giving us a stronger prediction than one tree could independently.\nSpecify model set.seed(100) mod_bag \u0026lt;- bag_tree() %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) %\u0026gt;% set_engine(\u0026quot;rpart\u0026quot;, times = 10) # 10 bootstrap resamples  Create workflow wflow_bag \u0026lt;- workflow() %\u0026gt;% add_recipe(rec) %\u0026gt;% add_model(mod_bag)  Fit the model set.seed(100) plan(multisession) fit_bag \u0026lt;- fit_resamples( wflow_bag, cv, metrics = metric_set(rmse, rsq), control = control_resamples(verbose = TRUE, save_pred = TRUE, extract = function(x) extract_model(x)))  Visualize The plot below shows the root nodes from a bagged tree made of 100 trees (10 folds x 10 bootstrapped resamples). Root nodes are the 1st node in a decision tree, and they are determined by which variable best optimizes a loss function (e.g., minimizes mean square error [MSE] for continuous outcomes or Gini Index for categorical outcomes). Put roughly, the most common root nodes can be thought of as the most “important” predictors.\n# extract roots bag_roots \u0026lt;- function(x){ x %\u0026gt;% select(.extracts) %\u0026gt;% unnest(cols = c(.extracts)) %\u0026gt;% mutate(models = map(.extracts, ~.x$model_df)) %\u0026gt;% select(-.extracts) %\u0026gt;% unnest(cols = c(models)) %\u0026gt;% mutate(root = map_chr(model, ~as.character(.x$fit$frame[1, 1]))) %\u0026gt;% select(root) } # plot bag_roots(fit_bag) %\u0026gt;% ggplot(mapping = aes(x = fct_rev(fct_infreq(root)))) + geom_bar() + coord_flip() + labs(x = \u0026quot;root\u0026quot;, y = \u0026quot;count\u0026quot;)   Random forest Random forest is similar to bagged tree methodology but goes one step further. In addition to taking random subsets of data, the model also draws a random selection of features. Instead of utilizing all features, the random subset of features allows more predictors to be eligible root nodes. This is particularly useful for handling high dimensionality data (e.g., have more variables than participants/cases).\nSpecify the model set.seed(100) mod_rf \u0026lt;-rand_forest() %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;, num.threads = parallel::detectCores(), importance = \u0026quot;permutation\u0026quot;, verbose = TRUE) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;) %\u0026gt;% set_args(trees = 1000)  Create workflow wflow_rf \u0026lt;- workflow() %\u0026gt;% add_model(mod_rf) %\u0026gt;% add_recipe(rec)  Fit the model set.seed(100) plan(multisession) fit_rf \u0026lt;- fit_resamples( wflow_rf, cv, metrics = metric_set(rmse, rsq), control = control_resamples(verbose = TRUE, save_pred = TRUE, extract = function(x) x) )  Visualize The plot below shows the root nodes from a random forest with 1000 trees (specified using set_args(trees = 1000) in the parsnip model object).\n# extract roots rf_tree_roots \u0026lt;- function(x){ map_chr(1:1000, ~ranger::treeInfo(x, tree = .)[1, \u0026quot;splitvarName\u0026quot;]) } rf_roots \u0026lt;- function(x){ x %\u0026gt;% select(.extracts) %\u0026gt;% unnest(cols = c(.extracts)) %\u0026gt;% mutate(fit = map(.extracts, ~.x$fit$fit$fit), oob_rmse = map_dbl(fit, ~sqrt(.x$prediction.error)), roots = map(fit, ~rf_tree_roots(.)) ) %\u0026gt;% select(roots) %\u0026gt;% unnest(cols = c(roots)) } # plot rf_roots(fit_rf) %\u0026gt;% group_by(roots) %\u0026gt;% count() %\u0026gt;% arrange(desc(n)) %\u0026gt;% filter(n \u0026gt; 75) %\u0026gt;% ggplot(aes(fct_reorder(roots, n), n)) + geom_col() + coord_flip() + labs(x = \u0026quot;root\u0026quot;, y = \u0026quot;count\u0026quot;)   Boosted trees Boosted trees, like bagged trees, are an ensemble model. Instead of applying successive models to resampled data and pooling estimates, boosted trees fit the next tree to the residuals (i.e., error term) of the prior tree. The goal is to minimize residual error through multiple trees, and is typically done with fairly “shallow” decision tree (i.e., 1-6 splits in each tree). Though each model is only slightly improving the error rate, the sequential use of many shallow trees makes computationally efficient (i.e. reduced run time) and highly accurate predictions.\nSpecify the model mod_boost \u0026lt;- boost_tree() %\u0026gt;% set_engine(\u0026quot;xgboost\u0026quot;, nthreads = parallel::detectCores()) %\u0026gt;% set_mode(\u0026quot;regression\u0026quot;)  Create workflow wflow_boost \u0026lt;- workflow() %\u0026gt;% add_recipe(rec) %\u0026gt;% add_model(mod_boost)  Fit the model set.seed(100) plan(multisession) fit_boost \u0026lt;- fit_resamples( wflow_boost, cv, metrics = metric_set(rmse, rsq), control = control_resamples(verbose = TRUE, save_pred = TRUE) )  Visualize One of the few downfalls of {tidymodels} is its (current) inability to plot these tree-based models. For the past two models, it was simpler to extract root nodes and plot them, but their interpretation (as we’re fitting to residuals instead of data sets) are not straightforward. For that reason, we don’t have any pretty plots here. Instead, we’ll skip to evaluating the metrics of all models.\n   Evaluate metrics After running these three models, it’s time to evaluate their performance. We can do this with tune::collect_metrics(). The table below shows the estimate of the out-of-sample performance for each of our 3 models.\ncollect_metrics(fit_bag) %\u0026gt;% bind_rows(collect_metrics(fit_rf)) %\u0026gt;% bind_rows(collect_metrics(fit_boost)) %\u0026gt;% filter(.metric == \u0026quot;rmse\u0026quot;) %\u0026gt;% mutate(model = c(\u0026quot;bag\u0026quot;, \u0026quot;rf\u0026quot;, \u0026quot;boost\u0026quot;)) %\u0026gt;% select(model, everything()) %\u0026gt;% knitr::kable()   model .metric .estimator mean n std_err    bag rmse standard 98.42890 10 2.504904  rf rmse standard 95.20828 10 3.466279  boost rmse standard 95.35888 10 2.764773    Here, we are faced with a common problem in the machine learning world: choosing between models that perform similarly (see overlapping standard errors). Whether we would prefer random forests or bagged trees may depend on computational efficiency (i.e., time) or other factors. In practice, tuning several hyperparameters may have made one model clearly preferable over the others, but in our case - relying on all defaults - we would probably have similar performance with both models on a new data set and would prefer random forest or boosted tree models for their efficiency.\n Out-of-sample performance The final step is to apply each trained model to our test data using last_fit().\n# bagged trees final_fit_bag \u0026lt;- last_fit( wflow_bag, split = split ) # random forest final_fit_rf \u0026lt;- last_fit( wflow_rf, split = split ) # boosted trees final_fit_boost \u0026lt;- last_fit( wflow_boost, split = split ) The table below shows the actual out-of-sample performance for each of our 3 models.\n# show performance on test data collect_metrics(final_fit_bag) %\u0026gt;% bind_rows(collect_metrics(final_fit_rf)) %\u0026gt;% bind_rows(collect_metrics(final_fit_boost)) %\u0026gt;% filter(.metric == \u0026quot;rmse\u0026quot;) %\u0026gt;% mutate(model = c(\u0026quot;bag\u0026quot;, \u0026quot;rf\u0026quot;, \u0026quot;boost\u0026quot;)) %\u0026gt;% select(model, everything()) %\u0026gt;% knitr::kable()   model .metric .estimator .estimate    bag rmse standard 93.36504  rf rmse standard 91.18114  boost rmse standard 94.22609    After applying our 3 trained models to the unseen test data, it looks like random forest is the winner since it has the lowest RMSE. In this example, we only used 1% of the data to train these models, which could make it difficult to meaningfully compare their performance. However, the random forest model also results in the best out-of-sample prediction (RMSE = 83.47).\n Session Info\n## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 4.0.2 (2020-06-22) ## os macOS Catalina 10.15.7 ## system x86_64, darwin17.0 ## ui X11 ## language (EN) ## collate da_DK.UTF-8 ## ctype da_DK.UTF-8 ## tz Europe/Copenhagen ## date 2020-11-24 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 4.0.2) ## backports 1.1.8 2020-06-17 [1] CRAN (R 4.0.2) ## baguette * 0.1.0 2020-10-28 [1] CRAN (R 4.0.2) ## base64enc 0.1-3 2015-07-28 [1] CRAN (R 4.0.2) ## blob 1.2.1 2020-01-20 [1] CRAN (R 4.0.2) ## blogdown 0.21 2020-10-11 [1] CRAN (R 4.0.2) ## bookdown 0.20 2020-06-23 [1] CRAN (R 4.0.2) ## broom * 0.7.0 2020-07-09 [1] CRAN (R 4.0.2) ## C50 0.1.3.1 2020-05-26 [1] CRAN (R 4.0.2) ## cellranger 1.1.0 2016-07-27 [1] CRAN (R 4.0.2) ## class 7.3-17 2020-04-26 [1] CRAN (R 4.0.2) ## cli 2.0.2 2020-02-28 [1] CRAN (R 4.0.2) ## codetools 0.2-16 2018-12-24 [1] CRAN (R 4.0.2) ## colorspace 1.4-1 2019-03-18 [1] CRAN (R 4.0.2) ## corrplot 0.84 2017-10-16 [1] CRAN (R 4.0.2) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 4.0.2) ## Cubist 0.2.3 2020-01-10 [1] CRAN (R 4.0.2) ## data.table 1.13.0 2020-07-24 [1] CRAN (R 4.0.2) ## DBI 1.1.0 2019-12-15 [1] CRAN (R 4.0.2) ## dbplyr 1.4.4 2020-05-27 [1] CRAN (R 4.0.2) ## dials * 0.0.9 2020-09-16 [1] CRAN (R 4.0.2) ## DiceDesign 1.8-1 2019-07-31 [1] CRAN (R 4.0.2) ## digest 0.6.25 2020-02-23 [1] CRAN (R 4.0.2) ## dplyr * 1.0.2 2020-08-18 [1] CRAN (R 4.0.2) ## earth 5.3.0 2020-10-11 [1] CRAN (R 4.0.2) ## ellipsis 0.3.1 2020-05-15 [1] CRAN (R 4.0.2) ## evaluate 0.14 2019-05-28 [1] CRAN (R 4.0.1) ## fansi 0.4.1 2020-01-08 [1] CRAN (R 4.0.2) ## farver 2.0.3 2020-01-16 [1] CRAN (R 4.0.2) ## forcats * 0.5.0 2020-03-01 [1] CRAN (R 4.0.2) ## foreach 1.5.1 2020-10-15 [1] CRAN (R 4.0.2) ## Formula 1.2-4 2020-10-16 [1] CRAN (R 4.0.2) ## fs 1.5.0 2020-07-31 [1] CRAN (R 4.0.2) ## furrr 0.2.1 2020-10-21 [1] CRAN (R 4.0.2) ## future * 1.20.1 2020-11-03 [1] CRAN (R 4.0.2) ## generics 0.1.0 2020-10-31 [1] CRAN (R 4.0.2) ## ggplot2 * 3.3.2 2020-06-19 [1] CRAN (R 4.0.2) ## globals 0.13.1 2020-10-11 [1] CRAN (R 4.0.2) ## glue 1.4.1 2020-05-13 [1] CRAN (R 4.0.2) ## gower 0.2.2 2020-06-23 [1] CRAN (R 4.0.2) ## GPfit 1.0-8 2019-02-08 [1] CRAN (R 4.0.2) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 4.0.2) ## hardhat 0.1.5 2020-11-09 [1] CRAN (R 4.0.2) ## haven 2.3.1 2020-06-01 [1] CRAN (R 4.0.2) ## highr 0.8 2019-03-20 [1] CRAN (R 4.0.2) ## hms 0.5.3 2020-01-08 [1] CRAN (R 4.0.2) ## htmltools 0.5.0 2020-06-16 [1] CRAN (R 4.0.2) ## httr 1.4.2 2020-07-20 [1] CRAN (R 4.0.2) ## infer * 0.5.3 2020-07-14 [1] CRAN (R 4.0.2) ## inum 1.0-1 2019-04-25 [1] CRAN (R 4.0.2) ## ipred 0.9-9 2019-04-28 [1] CRAN (R 4.0.2) ## iterators 1.0.13 2020-10-15 [1] CRAN (R 4.0.2) ## jsonlite 1.7.0 2020-06-25 [1] CRAN (R 4.0.2) ## knitr 1.29 2020-06-23 [1] CRAN (R 4.0.2) ## labeling 0.3 2014-08-23 [1] CRAN (R 4.0.2) ## lattice 0.20-41 2020-04-02 [1] CRAN (R 4.0.2) ## lava 1.6.8.1 2020-11-04 [1] CRAN (R 4.0.2) ## lhs 1.1.1 2020-10-05 [1] CRAN (R 4.0.2) ## libcoin 1.0-6 2020-08-14 [1] CRAN (R 4.0.2) ## lifecycle 0.2.0 2020-03-06 [1] CRAN (R 4.0.2) ## listenv 0.8.0 2019-12-05 [1] CRAN (R 4.0.2) ## lubridate 1.7.9 2020-06-08 [1] CRAN (R 4.0.2) ## magrittr 1.5 2014-11-22 [1] CRAN (R 4.0.2) ## MASS 7.3-52 2020-08-18 [1] CRAN (R 4.0.2) ## Matrix 1.2-18 2019-11-27 [1] CRAN (R 4.0.2) ## modeldata * 0.1.0 2020-10-22 [1] CRAN (R 4.0.2) ## modelr 0.1.8 2020-05-19 [1] CRAN (R 4.0.2) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 4.0.2) ## mvtnorm 1.1-1 2020-06-09 [1] CRAN (R 4.0.2) ## nnet 7.3-14 2020-04-26 [1] CRAN (R 4.0.2) ## parallelly 1.21.0 2020-10-27 [1] CRAN (R 4.0.2) ## parsnip * 0.1.4 2020-10-27 [1] CRAN (R 4.0.2) ## partykit 1.2-10 2020-10-12 [1] CRAN (R 4.0.2) ## pillar 1.4.6 2020-07-10 [1] CRAN (R 4.0.2) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 4.0.2) ## plotmo 3.6.0 2020-09-13 [1] CRAN (R 4.0.2) ## plotrix 3.7-8 2020-04-16 [1] CRAN (R 4.0.2) ## plyr 1.8.6 2020-03-03 [1] CRAN (R 4.0.2) ## pROC 1.16.2 2020-03-19 [1] CRAN (R 4.0.2) ## prodlim 2019.11.13 2019-11-17 [1] CRAN (R 4.0.2) ## purrr * 0.3.4 2020-04-17 [1] CRAN (R 4.0.2) ## R6 2.4.1 2019-11-12 [1] CRAN (R 4.0.2) ## ranger 0.12.1 2020-01-10 [1] CRAN (R 4.0.2) ## Rcpp 1.0.5 2020-07-06 [1] CRAN (R 4.0.2) ## readr * 1.3.1 2018-12-21 [1] CRAN (R 4.0.2) ## readxl 1.3.1 2019-03-13 [1] CRAN (R 4.0.2) ## recipes * 0.1.15 2020-11-11 [1] CRAN (R 4.0.2) ## repr 1.1.0 2020-01-28 [1] CRAN (R 4.0.2) ## reprex 0.3.0 2019-05-16 [1] CRAN (R 4.0.2) ## reshape2 1.4.4 2020-04-09 [1] CRAN (R 4.0.2) ## rlang 0.4.7 2020-07-09 [1] CRAN (R 4.0.2) ## rmarkdown 2.5 2020-10-21 [1] CRAN (R 4.0.2) ## rpart 4.1-15 2019-04-12 [1] CRAN (R 4.0.2) ## rsample * 0.0.8 2020-09-23 [1] CRAN (R 4.0.2) ## rstudioapi 0.11 2020-02-07 [1] CRAN (R 4.0.2) ## rvest 0.3.6 2020-07-25 [1] CRAN (R 4.0.2) ## scales * 1.1.1 2020-05-11 [1] CRAN (R 4.0.2) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 4.0.2) ## skimr * 2.1.2 2020-07-06 [1] CRAN (R 4.0.2) ## stringi 1.4.6 2020-02-17 [1] CRAN (R 4.0.2) ## stringr * 1.4.0 2019-02-10 [1] CRAN (R 4.0.2) ## survival 3.2-3 2020-06-13 [1] CRAN (R 4.0.2) ## TeachingDemos 2.12 2020-04-07 [1] CRAN (R 4.0.2) ## tibble * 3.0.3 2020-07-10 [1] CRAN (R 4.0.2) ## tidymodels * 0.1.1 2020-07-14 [1] CRAN (R 4.0.2) ## tidyr * 1.1.1 2020-07-31 [1] CRAN (R 4.0.2) ## tidyselect 1.1.0 2020-05-11 [1] CRAN (R 4.0.2) ## tidyverse * 1.3.0 2019-11-21 [1] CRAN (R 4.0.2) ## timeDate 3043.102 2018-02-21 [1] CRAN (R 4.0.2) ## tune * 0.1.2 2020-11-17 [1] CRAN (R 4.0.2) ## vctrs 0.3.2 2020-07-15 [1] CRAN (R 4.0.2) ## withr 2.2.0 2020-04-20 [1] CRAN (R 4.0.2) ## workflows * 0.2.1 2020-10-08 [1] CRAN (R 4.0.2) ## xfun 0.19 2020-10-30 [1] CRAN (R 4.0.2) ## xgboost * 1.2.0.1 2020-09-02 [1] CRAN (R 4.0.2) ## xml2 1.3.2 2020-04-23 [1] CRAN (R 4.0.2) ## yaml 2.2.1 2020-02-01 [1] CRAN (R 4.0.2) ## yardstick * 0.0.7 2020-07-13 [1] CRAN (R 4.0.2) ## ## [1] /Library/Frameworks/R.framework/Versions/4.0/Resources/library   ","date":1591056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591120643,"objectID":"05e567d1c7842451baa71ee85c492248","permalink":"/post/2020-06-02-tidymodels-decision-tree-learning-in-r/","publishdate":"2020-06-02T00:00:00Z","relpermalink":"/post/2020-06-02-tidymodels-decision-tree-learning-in-r/","section":"post","summary":"An overview and worked example of tree-based machine learning methods in R using `tidymodels`","tags":["R"],"title":"Tidymodels: Decision Tree Learning in R","type":"post"},{"authors":null,"categories":null,"content":"After the financial crisis the world economics saw it self in the deepest recessions since 1939. For comming out of this crisis the Euopean Central Bank (ECB) tried a alternative tool called Quantitative easing. It was implementet as a way to secure price stability which is ECB main agenda. In this analysis I try to quantify weather they made their goal and as a side effect managed to lift the economics. By measuring this I am using a vector autoregressiv (VAR) model.\n","date":1587686400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587686400,"objectID":"a1a25aab2012fb7a44fd1a1c7a39a64b","permalink":"/project/qe/","publishdate":"2020-04-24T00:00:00Z","relpermalink":"/project/qe/","section":"project","summary":"Going thought a VAR model to access the effect of QE","tags":["Economics","R"],"title":"Did QE effect the economics","type":"project"},{"authors":[],"categories":["logistic regression","random forest","tidymodels","parship"],"content":"  Introduktion I denne post vil jeg kigger på to modeller. Den logistiske regression og random forest, hvor de begge bliver brugtblog som klassifikations modeller.\nJeg kommer til at gennemgå og beskrive Random Forest da det er en model, som er forholdsvis ny for mig og der er nogle teoretiske framwork jeg gerne vil prøve at forklare.\nDesuden kommer vi til at stifte bekendtskab med parsnip som gør det let at skifte om til de forskellige modeller. Med det nye framwork fra tidymodels kan man skifte utrolig let fra glm til en cross validated random forest med ranger med få linjers koder.\n Random forest Det er en af de mest populære machine learning algoritmer og kan både bruges som en regresssion og klassifikation model.\nSom navnet antyder så laver algoritmen en skov med forskellige beslutningstræer. Desto flere træer desto mere robust er modellen. Navnet random kommer grundet to koncepter\nEt randomiseret sample af trænings data, når man bygger hver enkelt træ. Et randomiseret subsæt af features, når man splitter noder.  Når vi træer hver træ så lærer den fra et random sample af data punkter. Samples er trukket med erstatning, som kaldes bootstrapping, som betyder at et sample vil blive brugt flere gange i et enkelt træ. Ideen er at ved at træne hver træ med forskellige samples, så vil vi få en lavere varians og ikke få et højere bias.\nEns prediction fås ved at tage gennmsnittet af predictor for hver beslutningstræ. Denne procedure kaldes for bagging.\nFordele er man kan bruge den som klassifikation og regression. Den vil ikke overfitte. Den kan håndtere store datasæt med mange dimensioner.\nUlemper er den ikke er så god til regressioner. Den er ikke god til at forudsige. Der er heller ikke meget kontrol over modellen.\nDog er modellen anvendelig i mange sektor såsom banker, forsikringsselskaber, forretninger somkan bruges til at finde de loyolae kunder. Den kan også bruges i aktiemarkedet til ast finde opførelsen af en aktie.\n Data I dette projekt bruger jeg data fra Telco Customer Churn. Data indeholder 7043 rækker som hver repræsentere en kunde. Der er 21 kolonner som er mulige predictor, der giver information til vi kan forecast opførelse og give indsigt på forebyggelsesprogrammer.\nChurn er den afhængige variable og viser om kunden har forladt virksomheden indenfor den seneste måned.\nJeg bruger funnktionen skim til at skabe et overblik over mit data.\ntelco \u0026lt;- readr::read_csv(\u0026quot;https://raw.githubusercontent.com/treselle-systems/customer_churn_analysis/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) telco %\u0026gt;% skimr::skim()  Table 1: Data summary  Name Piped data  Number of rows 7043  Number of columns 21  _______________________   Column type frequency:   character 17  numeric 4  ________________________   Group variables None    Variable type: character\n  skim_variable n_missing complete_rate min max empty n_unique whitespace    customerID 0 1 10 10 0 7043 0  gender 0 1 4 6 0 2 0  Partner 0 1 2 3 0 2 0  Dependents 0 1 2 3 0 2 0  PhoneService 0 1 2 3 0 2 0  MultipleLines 0 1 2 16 0 3 0  InternetService 0 1 2 11 0 3 0  OnlineSecurity 0 1 2 19 0 3 0  OnlineBackup 0 1 2 19 0 3 0  DeviceProtection 0 1 2 19 0 3 0  TechSupport 0 1 2 19 0 3 0  StreamingTV 0 1 2 19 0 3 0  StreamingMovies 0 1 2 19 0 3 0  Contract 0 1 8 14 0 3 0  PaperlessBilling 0 1 2 3 0 2 0  PaymentMethod 0 1 12 25 0 4 0  Churn 0 1 2 3 0 2 0    Variable type: numeric\n  skim_variable n_missing complete_rate mean sd p0 p25 p50 p75 p100 hist    SeniorCitizen 0 1 0.16 0.37 0.00 0.00 0.00 0.00 1.00 ▇▁▁▁▂  tenure 0 1 32.37 24.56 0.00 9.00 29.00 55.00 72.00 ▇▃▃▃▆  MonthlyCharges 0 1 64.76 30.09 18.25 35.50 70.35 89.85 118.75 ▇▅▆▇▅  TotalCharges 11 1 2283.30 2266.77 18.80 401.45 1397.47 3794.74 8684.80 ▇▂▂▂▁    Her er en række ting at lægge mærke til her.\n customerID er en unik id for hver række og af den grund har den ingen deskriptiv eller predictive power og den skal fjernes. Der er meget få NA værdier, så de kan jeg tillade mig at slette.  telco \u0026lt;- telco %\u0026gt;% select(-customerID) %\u0026gt;% drop_na()  Modellering med tidymodels Denne post giver også en introduktion til tidymodels. Derfor vil modellen være simpel og kommer til at bestå af logistic regression model uden meget data bearbejdring.\n Train and test split rsample() kan bruges til at lave en randomiserede træning og test data, som selvfølgelig er konstrueret udfra vores orginale telco data.\nset.seed(1972) train_test_split \u0026lt;- rsample::initial_split( data = telco, prop = 0.8 ) train_test_split ## \u0026lt;Analysis/Assess/Total\u0026gt; ## \u0026lt;5626/1406/7032\u0026gt; Ud fra ovenstående har vi at de 7032 kunder er blevet delt ud, og de 5626 er blevet sat i træningssættet. Vi gemmer dem ned i deres eget data frame;\ntrain_tbl \u0026lt;- train_test_split %\u0026gt;% training() %\u0026gt;% unnest() test_tbl \u0026lt;- train_test_split %\u0026gt;% testing()  En bage opskrift For at lave en del af arbejde for at bygge modellen bruger vi recipe(). Denne pakke bruger bage metafor til at behandle data og foretage diverse præprocessor såsom, missing values, fjerne predictor, centering og scaling osv..\nDet første man gør er at definere recipe og de transformationer man vil bruge på ens data. Der er ikke meget at gøre i dette tilfælde, udover at tranaformerer til faktor.\nrecipe_simple \u0026lt;- function(dataset) { recipe(Churn ~ ., data = dataset) %\u0026gt;% step_string2factor(all_nominal(), -all_outcomes()) %\u0026gt;% prep(data = dataset) } For at undgår man vi har en data lækage (oveføre information mellem træning og test data), skal data være ‘prepped’ ved kun at bruge train_tbl.\nrecipe_prepped \u0026lt;- recipe_simple(dataset = train_tbl) Som den sidste del så skal vi bage opskriften for at alle præprocessor bliver inkluderet i data sættene.\ntrain_baked \u0026lt;- bake(recipe_prepped, new_data = train_tbl) test_baked \u0026lt;- bake(recipe_prepped, new_data = test_tbl)  Fit modellen Tidymodels er det helt nye indspark fra tidyverse folkene på at skabe et framwork for machine learning. Hertil er der blevet lavet en del justeringer og nye pakker. En central pakke i dette framwork er parsnip,som skaber en adgang til mange machine learning pakker uden man skal kunne syntaksen til dem alle.\nMan skal følge tre trin:\nBestem typen af modellen og mode. Bestem engine. Bestem model specifikationer og data der skal bruges.  logistic_glm \u0026lt;- logistic_reg(mode = \u0026quot;classification\u0026quot;) %\u0026gt;% set_engine(\u0026quot;glm\u0026quot;) %\u0026gt;% fit(Churn ~ ., data = train_baked) Som sagt så kan du vælge en masse andre engine. I dette tilfælde hvor vi bruge en logistisk regression, så kan vi vælge; glm, glmnet, stan, spark og keras. Det smarte er vi bare kan skifte det ud og så klare parsnip transitionen.\n Hvor godt klare modellen sig? Det er væsentlig at se hvor god modellen er og her bruger vi pakken yardstick, som gør det let at beregne forskellige måleværktøjer. Før man kan beregne disse måle enheder skal vi beregne nogle predictor ved at bruge test_baked til predict funktionen.\nprediction_glm \u0026lt;- logistic_glm %\u0026gt;% predict(new_data = test_baked) %\u0026gt;% bind_cols(test_baked %\u0026gt;% select(Churn)) head(prediction_glm) ## # A tibble: 6 x 2 ## .pred_class Churn ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Yes No ## 2 No No ## 3 No No ## 4 No No ## 5 No No ## 6 No No Der kan benyttes mange matricer til at undersøge hvor god modellen er, men fokus for denne post bliver accuracy, precision, recall og F1_score.\nDisse mål bliver udledt af Confusion Matrix, som er en tabel der beskriver hvor godt ens klassifikations model klarer sig. Denne matrice er i sig selv ikke svær at forstå, da den angiver antallet af; false positives, false negatives, true positives og true negatives. Dog er nogle af målene, som udledes herfra svære koncepter og kræver reflektion for at forstå deres betydning.\nprediction_glm %\u0026gt;% conf_mat(Churn, .pred_class) %\u0026gt;% pluck(1) %\u0026gt;% as_tibble() %\u0026gt;% ggplot(aes(Prediction, Truth, alpha = n)) + geom_tile(show.legend = FALSE) + geom_text(aes(label = n), colour = \u0026quot;white\u0026quot;, alpha = 0.5, size = 12) Modellen Accuracy er andel af prediction modellen ramte plet og kan udregnes ved at lade predictions_glm gå gennem metrics funktionen. Dog er den ikke så troværdig, hvis ens data er ubalanceret.\nprediction_glm %\u0026gt;% metrics(Churn, .pred_class) %\u0026gt;% select(-.estimator) %\u0026gt;% filter(.metric == \u0026quot;accuracy\u0026quot;) ## # A tibble: 1 x 2 ## .metric .estimate ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 accuracy 0.806 Modellen får altså en score på 78%.\nPrecision målser hvor sensitiv modellen er overfor False Positive, mens Recall ser hvor sensitiv modellen er for False Negative.\nDisse metricer er meget vigtig informationer for virksomheder fordi man så kan forudsige hvilke kunder der er i en risiko gruppe for at forlade forretningen. Herfra kan man så benytte sig af en fastholdessstrategi. Desuen kan man bruge oplysning til ikke at bruge penge på kudner der alligevel har tænkt sig at forlade virksomheden.\ntibble( \u0026quot;precision\u0026quot; = precision(prediction_glm, Churn, .pred_class) %\u0026gt;% select(.estimate), \u0026quot;recall\u0026quot; = recall(prediction_glm, Churn, .pred_class) %\u0026gt;% select(.estimate) ) %\u0026gt;% unnest() %\u0026gt;% kable()   precision recall    0.8466368 0.9024857    Den anden og sidste populær måleværktøj er F1_score, som er det harmoniske gennemsnit af precision og recall. Den perfekte score på 1 fås når precision og recall er perfekte.\nprediction_glm %\u0026gt;% f_meas(Churn, .pred_class) %\u0026gt;% select(-.estimator) %\u0026gt;% kable()   .metric .estimate    f_meas 0.8736696     Fra logitstik regression til Random Forest Det er utrolig simpel at skifte ens model ud med en anden. Den tidligere anvendte logistisk regressions model kan vi hurtig skifte ud med en Random Forest model med ranger.\n Croos validation sæt op For at styke modellens prediktive kræft kan man foretage cross validation, som tit bliver sat op med 10 folder. Det kan implementeres med vfold_cv() fra rsample, som splitter det initale trænings data.\nset.seed(123) cross_val_tbl \u0026lt;- vfold_cv(train_tbl, v = 10) Vi kan genkende de 5626 fra vores tærningssæt. I hver runde vil 563 observationer blive brugt til validere modellen for det specifikke fold.\nFor at ikke blive forvirret over bruget af initial træsning/test split til det man bruger i cross validation benytter man begreberne analysis (estimer modellen) og assessment (valider estimater).\n Opdater recipe For at bruge Random Forest skal alle numeriske værdier være centred og scaled og alle faktor skal være dummies.\nsplit \u0026lt;- initial_split(telco, prop = 0.8) train_data \u0026lt;- training(split) test_data \u0026lt;- testing(split) For at skifte over til en anden model er utroligt simepel. Her ændre vi til random forest i typen af modellen og tilføjer dens hyperparameter.\nFor at gøre processen lidt hurtigere propper jeg det hele i en funktion, som estimer modellen på tværs af alle folder og retuner det i en tibble. Desuden skal der tilføjes et skridt mere for at vi mapper de forskellige folder.\nrecipe_rf \u0026lt;- function(dataset) { recipe(Churn ~ ., data = dataset) %\u0026gt;% step_string2factor(all_nominal(), -all_outcomes()) %\u0026gt;% step_dummy(all_nominal(), -all_outcomes()) %\u0026gt;% step_center(all_numeric()) %\u0026gt;% step_scale(all_numeric()) %\u0026gt;% prep(data = dataset) } rf_fun \u0026lt;- function(split, id, try, tree) { analysis_set \u0026lt;- split %\u0026gt;% analysis() analysis_prepped \u0026lt;- analysis_set %\u0026gt;% recipe_rf() analysis_baked \u0026lt;- analysis_prepped %\u0026gt;% bake(new_data = analysis_set) model_rf \u0026lt;- rand_forest( mode = \u0026quot;classification\u0026quot;, mtry = try, trees = tree ) %\u0026gt;% set_engine(\u0026quot;ranger\u0026quot;, importance = \u0026quot;impurity\u0026quot; ) %\u0026gt;% fit(Churn ~ ., data = analysis_baked) assessment_set \u0026lt;- split %\u0026gt;% assessment() assessment_prepped \u0026lt;- assessment_set %\u0026gt;% recipe_rf() assessment_baked \u0026lt;- assessment_prepped %\u0026gt;% bake(new_data = assessment_set) tibble( \u0026quot;id\u0026quot; = id, \u0026quot;truth\u0026quot; = assessment_baked$Churn, \u0026quot;prediction\u0026quot; = model_rf %\u0026gt;% predict(new_data = assessment_baked) %\u0026gt;% unlist() ) } pred_rf \u0026lt;- map2_df( .x = cross_val_tbl$splits, .y = cross_val_tbl$id, ~ rf_fun(split = .x, id = .y, try = 3, tree = 200) ) head(pred_rf)  ## # A tibble: 6 x 3 ## id truth prediction ## \u0026lt;chr\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Fold01 Yes No ## 2 Fold01 Yes Yes ## 3 Fold01 No No ## 4 Fold01 No No ## 5 Fold01 No No ## 6 Fold01 No No pred_rf %\u0026gt;% conf_mat(truth, prediction) %\u0026gt;% summary() %\u0026gt;% select(-.estimator) %\u0026gt;% filter(.metric %in% c(\u0026quot;accuracy\u0026quot;, \u0026quot;precision\u0026quot;, \u0026quot;recall\u0026quot;, \u0026quot;f_meas\u0026quot;)) %\u0026gt;% kable()   .metric .estimate    accuracy 0.7996801  precision 0.8291502  recall 0.9147437  f_meas 0.8698464    Der er mange matricer til at validere vores model, men vi bruger dem som vi brugte ved vores logistisk regression.\nModellen klare sig på lige fod med regressionsmodellen. Man kunne gå tilbage til modellen og laver yderligere feature eengierning da det ville gøre noget for selve præcisionen af modellen.\n ","date":1581897600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1589739594,"objectID":"2dd1ed83472053f5e0c15ff91faaa973","permalink":"/post/tidymodels-and-parsnip/","publishdate":"2020-02-17T00:00:00Z","relpermalink":"/post/tidymodels-and-parsnip/","section":"post","summary":"Introduktion I denne post vil jeg kigger på to modeller. Den logistiske regression og random forest, hvor de begge bliver brugtblog som klassifikations modeller.\nJeg kommer til at gennemgå og beskrive Random Forest da det er en model, som er forholdsvis ny for mig og der er nogle teoretiske framwork jeg gerne vil prøve at forklare.","tags":["tidymodels","ML","parship","random forest"],"title":"Tidymodels, Random Forest og parsnip","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic  Academic | Documentation  Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click  PDF Export : E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions?  Ask  Documentation ","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6d99026b9e19e4fa43d5aadf147c7176","permalink":"/contact/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/contact/","section":"","summary":"Hello!","tags":null,"title":"Landing Page","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"077e1e01fd8590c86aff39607410dbcb","permalink":"/project_landing/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/project_landing/","section":"","summary":"Hello!","tags":null,"title":"Projects","type":"widget_page"}]